{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a5057a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "References:\n",
    "https://arxiv.org/pdf/1512.03385.pdf\n",
    "https://arxiv.org/pdf/1604.04112v4.pdf\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Add, Dense, Conv2D, BatchNormalization,ReLU\n",
    "from tensorflow.keras.layers import Activation, AveragePooling2D, Input, Flatten,MaxPool2D\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler,TensorBoard,EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from cutout_generator import CutoutImageDataGenerator\n",
    "from mixup_generator import MixupGenerator\n",
    "from cutmix_generator import CutMixGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d848885",
   "metadata": {},
   "source": [
    "# hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e34ed2d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 120\n",
    "n_classes = 100\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b827ca",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30c65c3d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000, 100) (10000, 32, 32, 3) (10000, 100)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "y_train = to_categorical(y_train, n_classes)\n",
    "y_test = to_categorical(y_test, n_classes)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)           #(50000, 32, 32, 3) (50000, 100) (10000, 32, 32, 3) (10000,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867f60f1",
   "metadata": {},
   "source": [
    "# normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "407bd7a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(129.30428, 124.07023, 112.43411)\n",
      "(68.17024, 65.391785, 70.4184)\n"
     ]
    }
   ],
   "source": [
    "def get_mean_std(images):\n",
    "    images = images.astype(np.float32)\n",
    "    mean_vals = ()\n",
    "    std_vals = ()\n",
    "    for i in range(images.shape[-1]):\n",
    "        mean_vals += (np.mean(images[:, :, :, i]),)\n",
    "        std_vals += (np.std(images[:, :, :, i]),)\n",
    "    return mean_vals, std_vals\n",
    "\n",
    "mean_vals, std_vals = get_mean_std(x_train)\n",
    "print(mean_vals)\n",
    "print(std_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5976f33",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0245545 -1.8973366\n"
     ]
    }
   ],
   "source": [
    "def normalize(images, mean_vals, std_vals):\n",
    "    images = images.astype(np.float32)\n",
    "    for i in range(images.shape[-1]):\n",
    "        images[:, :, :, i] = (images[:, :, :, i] - mean_vals[i])/std_vals[i]\n",
    "    return images\n",
    "\n",
    "x_train = normalize(x_train, mean_vals, std_vals)\n",
    "x_test = normalize(x_test, mean_vals, std_vals)\n",
    "print(x_test.max(), x_test.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeb897f",
   "metadata": {},
   "source": [
    "# divide train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e32910c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 32, 32, 3) (40000, 100) (10000, 32, 32, 3) (10000, 100)\n"
     ]
    }
   ],
   "source": [
    "val_frac = 0.2\n",
    "perm_ids = np.random.permutation(x_train.shape[0])\n",
    "val_ids = perm_ids[:int(val_frac*x_train.shape[0])]\n",
    "train_ids = perm_ids[int(val_frac*x_train.shape[0]):]\n",
    "x_val, y_val = x_train[val_ids], y_train[val_ids]\n",
    "x_train, y_train = x_train[train_ids], y_train[train_ids]\n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390a1688",
   "metadata": {},
   "source": [
    "# network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db844d1b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Resnet:\n",
    "    def __init__(self, size=56, stacks=3, starting_filter=16):\n",
    "        self.size = size\n",
    "        self.stacks = stacks\n",
    "        self.starting_filter = starting_filter\n",
    "        self.residual_blocks = (size - 2) // 6\n",
    "        \n",
    "    def get_model(self, input_shape=(32, 32, 3), n_classes=100):\n",
    "        n_filters = self.starting_filter\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "        network = self.layer(inputs, n_filters)\n",
    "        network = self.stack(network, n_filters, True)\n",
    "\n",
    "        for _ in range(self.stacks - 1):\n",
    "            n_filters *= 2\n",
    "            network = self.stack(network, n_filters)\n",
    "\n",
    "        network = Activation('elu')(network)\n",
    "        pd=network.shape[1]\n",
    "        network = AveragePooling2D(pool_size=(pd,pd))(network)\n",
    "        network = Flatten()(network)\n",
    "        outputs = Dense(n_classes, activation='softmax', \n",
    "                        kernel_initializer='he_normal')(network)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def stack(self, inputs, n_filters, first_stack=False):\n",
    "        stack = inputs\n",
    "\n",
    "        if first_stack:\n",
    "            stack = self.identity_block(stack, n_filters)\n",
    "        else:\n",
    "            stack = self.convolution_block(stack, n_filters)\n",
    "\n",
    "        for _ in range(self.residual_blocks - 1):\n",
    "            stack = self.identity_block(stack, n_filters)\n",
    "\n",
    "        return stack\n",
    "    \n",
    "    def identity_block(self, inputs, n_filters):\n",
    "        shortcut = inputs\n",
    "\n",
    "        block = self.layer(inputs, n_filters, normalize_batch=False)\n",
    "        block = self.layer(block, n_filters, activation=None)\n",
    "\n",
    "        block = Add()([shortcut, block])\n",
    "\n",
    "        return block\n",
    "\n",
    "    def convolution_block(self, inputs, n_filters, strides=2):\n",
    "        shortcut = inputs\n",
    "\n",
    "        block = self.layer(inputs, n_filters, strides=strides,\n",
    "                           normalize_batch=False)\n",
    "        block = self.layer(block, n_filters, activation=None)\n",
    "\n",
    "        shortcut = self.layer(shortcut, n_filters,\n",
    "                              kernel_size=1, strides=strides,\n",
    "                              activation=None)\n",
    "\n",
    "        block = Add()([shortcut, block])\n",
    "\n",
    "        return block\n",
    "    \n",
    "    def layer(self, inputs, n_filters, kernel_size=3,\n",
    "              strides=1, activation='elu', normalize_batch=True):\n",
    "    \n",
    "        convolution = Conv2D(n_filters, kernel_size=kernel_size,\n",
    "                             strides=strides, padding='same',\n",
    "                             kernel_initializer=\"he_normal\",\n",
    "                             kernel_regularizer=l2(1e-4))\n",
    "\n",
    "        x = convolution(inputs)\n",
    "\n",
    "        if normalize_batch:\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3de5a3fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resnet = Resnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b662e33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MECHREVO\\Anaconda3\\envs\\tf1\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1 (BatchNo (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization_v1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   2320        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_1 (Batch (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 16)   0           activation[0][0]                 \n",
      "                                                                 batch_normalization_v1_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_2 (Batch (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           add[0][0]                        \n",
      "                                                                 batch_normalization_v1_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_3 (Batch (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "                                                                 batch_normalization_v1_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_4 (Batch (None, 32, 32, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "                                                                 batch_normalization_v1_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 16)   2320        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_5 (Batch (None, 32, 32, 16)   64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "                                                                 batch_normalization_v1_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 16)   2320        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_6 (Batch (None, 32, 32, 16)   64          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 32, 32, 16)   0           add_4[0][0]                      \n",
      "                                                                 batch_normalization_v1_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 16)   2320        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 16)   2320        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_7 (Batch (None, 32, 32, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 32, 32, 16)   0           add_5[0][0]                      \n",
      "                                                                 batch_normalization_v1_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 16)   2320        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 16)   0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_8 (Batch (None, 32, 32, 16)   64          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 32, 32, 16)   0           add_6[0][0]                      \n",
      "                                                                 batch_normalization_v1_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 16)   2320        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 16)   0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 32, 32, 16)   2320        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_9 (Batch (None, 32, 32, 16)   64          conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 32, 32, 16)   0           add_7[0][0]                      \n",
      "                                                                 batch_normalization_v1_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 32)   4640        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 32)   544         add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_11 (Batc (None, 16, 16, 32)   128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_10 (Batc (None, 16, 16, 32)   128         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 16, 16, 32)   0           batch_normalization_v1_11[0][0]  \n",
      "                                                                 batch_normalization_v1_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 32)   9248        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_12 (Batc (None, 16, 16, 32)   128         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 16, 16, 32)   0           add_9[0][0]                      \n",
      "                                                                 batch_normalization_v1_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 32)   9248        add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_13 (Batc (None, 16, 16, 32)   128         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 16, 16, 32)   0           add_10[0][0]                     \n",
      "                                                                 batch_normalization_v1_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 32)   9248        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 16, 16, 32)   9248        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_14 (Batc (None, 16, 16, 32)   128         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 16, 16, 32)   0           add_11[0][0]                     \n",
      "                                                                 batch_normalization_v1_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 32)   9248        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 32)   0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 16, 16, 32)   9248        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_15 (Batc (None, 16, 16, 32)   128         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 16, 16, 32)   0           add_12[0][0]                     \n",
      "                                                                 batch_normalization_v1_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 16, 16, 32)   9248        add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 32)   0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 16, 16, 32)   9248        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_16 (Batc (None, 16, 16, 32)   128         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 16, 16, 32)   0           add_13[0][0]                     \n",
      "                                                                 batch_normalization_v1_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 16, 16, 32)   9248        add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 32)   0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 16, 16, 32)   9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_17 (Batc (None, 16, 16, 32)   128         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 16, 16, 32)   0           add_14[0][0]                     \n",
      "                                                                 batch_normalization_v1_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 16, 16, 32)   9248        add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 32)   0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 16, 16, 32)   9248        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_18 (Batc (None, 16, 16, 32)   128         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 16, 16, 32)   0           add_15[0][0]                     \n",
      "                                                                 batch_normalization_v1_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 16, 16, 32)   9248        add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 32)   0           conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 16, 16, 32)   9248        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_19 (Batc (None, 16, 16, 32)   128         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 16, 16, 32)   0           add_16[0][0]                     \n",
      "                                                                 batch_normalization_v1_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 8, 8, 64)     18496       add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 8, 8, 64)     2112        add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 8, 8, 64)     36928       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_21 (Batc (None, 8, 8, 64)     256         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_20 (Batc (None, 8, 8, 64)     256         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 8, 8, 64)     0           batch_normalization_v1_21[0][0]  \n",
      "                                                                 batch_normalization_v1_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 8, 8, 64)     36928       add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 64)     0           conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 8, 8, 64)     36928       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_22 (Batc (None, 8, 8, 64)     256         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 8, 8, 64)     0           add_18[0][0]                     \n",
      "                                                                 batch_normalization_v1_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 8, 8, 64)     36928       add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 64)     0           conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 8, 8, 64)     36928       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_23 (Batc (None, 8, 8, 64)     256         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 8, 8, 64)     0           add_19[0][0]                     \n",
      "                                                                 batch_normalization_v1_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 8, 8, 64)     36928       add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 8, 8, 64)     0           conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 8, 8, 64)     36928       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_24 (Batc (None, 8, 8, 64)     256         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 8, 8, 64)     0           add_20[0][0]                     \n",
      "                                                                 batch_normalization_v1_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 8, 8, 64)     36928       add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 8, 8, 64)     0           conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 8, 8, 64)     36928       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_25 (Batc (None, 8, 8, 64)     256         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 8, 8, 64)     0           add_21[0][0]                     \n",
      "                                                                 batch_normalization_v1_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 8, 8, 64)     36928       add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 8, 8, 64)     0           conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 8, 8, 64)     36928       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_26 (Batc (None, 8, 8, 64)     256         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 8, 8, 64)     0           add_22[0][0]                     \n",
      "                                                                 batch_normalization_v1_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 8, 8, 64)     36928       add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 8, 8, 64)     0           conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 8, 8, 64)     36928       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_27 (Batc (None, 8, 8, 64)     256         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 8, 8, 64)     0           add_23[0][0]                     \n",
      "                                                                 batch_normalization_v1_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 8, 8, 64)     36928       add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 8, 8, 64)     0           conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 8, 8, 64)     36928       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_28 (Batc (None, 8, 8, 64)     256         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 8, 8, 64)     0           add_24[0][0]                     \n",
      "                                                                 batch_normalization_v1_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 8, 8, 64)     36928       add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 64)     0           conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 8, 8, 64)     36928       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v1_29 (Batc (None, 8, 8, 64)     256         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 8, 8, 64)     0           add_25[0][0]                     \n",
      "                                                                 batch_normalization_v1_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 64)     0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 1, 1, 64)     0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 64)           0           average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          6500        flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 863,972\n",
      "Trainable params: 861,732\n",
      "Non-trainable params: 2,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = resnet.get_model()\n",
    "\n",
    "optimizer = SGD(lr=learning_rate, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c86ea26",
   "metadata": {},
   "source": [
    "# Train Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c587195",
   "metadata": {},
   "source": [
    "### baseline witout data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e610e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "log= TensorBoard(log_dir=\"./logs/baseline\")\n",
    "\n",
    "callbacks = [log,EarlyStopping(monitor = 'val_loss', patience = 13), \n",
    "             ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 6, verbose = 1，min_lr=1e-4)]\n",
    "#datagen = ImageDataGenerator(width_shift_range=0.15,\n",
    "#                             height_shift_range=0.15,\n",
    "#                            horizontal_flip=True)\n",
    "datagen = ImageDataGenerator()\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "          validation_data=(x_val, y_val),\n",
    "          steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model.save('model_withoutaug.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612e0989",
   "metadata": {},
   "source": [
    "### baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9aad569b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "10000/10000 [==============================] - 5s 546us/sample - loss: 6.2454 - acc: 0.0673\n",
      "313/313 [==============================] - 58s 184ms/step - loss: 4.2958 - acc: 0.1075 - val_loss: 6.2511 - val_acc: 0.0673\n",
      "Epoch 2/120\n",
      "10000/10000 [==============================] - 4s 420us/sample - loss: 3.5558 - acc: 0.2278\n",
      "313/313 [==============================] - 52s 168ms/step - loss: 3.6654 - acc: 0.2040 - val_loss: 3.5579 - val_acc: 0.2278\n",
      "Epoch 3/120\n",
      "10000/10000 [==============================] - 4s 420us/sample - loss: 3.1426 - acc: 0.2936\n",
      "313/313 [==============================] - 54s 171ms/step - loss: 3.2917 - acc: 0.2699 - val_loss: 3.1450 - val_acc: 0.2936\n",
      "Epoch 4/120\n",
      "10000/10000 [==============================] - 4s 421us/sample - loss: 2.9226 - acc: 0.3420\n",
      "313/313 [==============================] - 54s 173ms/step - loss: 2.9961 - acc: 0.3282 - val_loss: 2.9291 - val_acc: 0.3420\n",
      "Epoch 5/120\n",
      "10000/10000 [==============================] - 4s 422us/sample - loss: 2.7868 - acc: 0.3764\n",
      "313/313 [==============================] - 54s 173ms/step - loss: 2.7848 - acc: 0.3719 - val_loss: 2.7893 - val_acc: 0.3764\n",
      "Epoch 6/120\n",
      "10000/10000 [==============================] - 4s 420us/sample - loss: 2.9430 - acc: 0.3527\n",
      "313/313 [==============================] - 54s 174ms/step - loss: 2.6044 - acc: 0.4107 - val_loss: 2.9430 - val_acc: 0.3527\n",
      "Epoch 7/120\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 2.7893 - acc: 0.3868\n",
      "313/313 [==============================] - 55s 174ms/step - loss: 2.4692 - acc: 0.4394 - val_loss: 2.7923 - val_acc: 0.3868\n",
      "Epoch 8/120\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 2.5859 - acc: 0.4265\n",
      "313/313 [==============================] - 55s 174ms/step - loss: 2.3456 - acc: 0.4673 - val_loss: 2.5838 - val_acc: 0.4265\n",
      "Epoch 9/120\n",
      "10000/10000 [==============================] - 4s 426us/sample - loss: 2.4898 - acc: 0.4409\n",
      "313/313 [==============================] - 55s 175ms/step - loss: 2.2502 - acc: 0.4912 - val_loss: 2.4905 - val_acc: 0.4409\n",
      "Epoch 10/120\n",
      "10000/10000 [==============================] - 4s 424us/sample - loss: 2.3993 - acc: 0.4750\n",
      "313/313 [==============================] - 55s 175ms/step - loss: 2.1542 - acc: 0.5139 - val_loss: 2.4035 - val_acc: 0.4750\n",
      "Epoch 11/120\n",
      "10000/10000 [==============================] - 4s 426us/sample - loss: 2.2824 - acc: 0.4969\n",
      "313/313 [==============================] - 55s 175ms/step - loss: 2.0792 - acc: 0.5332 - val_loss: 2.2817 - val_acc: 0.4969\n",
      "Epoch 12/120\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 2.2593 - acc: 0.5027\n",
      "313/313 [==============================] - 55s 175ms/step - loss: 2.0129 - acc: 0.5537 - val_loss: 2.2613 - val_acc: 0.5027\n",
      "Epoch 13/120\n",
      "10000/10000 [==============================] - 4s 431us/sample - loss: 2.3099 - acc: 0.5031\n",
      "313/313 [==============================] - 56s 177ms/step - loss: 1.9490 - acc: 0.5692 - val_loss: 2.3117 - val_acc: 0.5031\n",
      "Epoch 14/120\n",
      "10000/10000 [==============================] - 4s 422us/sample - loss: 2.2590 - acc: 0.5127\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.9186 - acc: 0.5806 - val_loss: 2.2586 - val_acc: 0.5127\n",
      "Epoch 15/120\n",
      "10000/10000 [==============================] - 4s 426us/sample - loss: 2.3823 - acc: 0.4920\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.8640 - acc: 0.5944 - val_loss: 2.3793 - val_acc: 0.4920\n",
      "Epoch 16/120\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 2.2421 - acc: 0.5291\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.8296 - acc: 0.6039 - val_loss: 2.2423 - val_acc: 0.5291\n",
      "Epoch 17/120\n",
      "10000/10000 [==============================] - 4s 427us/sample - loss: 2.2971 - acc: 0.5121\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.8081 - acc: 0.6119 - val_loss: 2.2969 - val_acc: 0.5121\n",
      "Epoch 18/120\n",
      "10000/10000 [==============================] - 4s 428us/sample - loss: 2.2743 - acc: 0.5187\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.7810 - acc: 0.6188 - val_loss: 2.2744 - val_acc: 0.5187\n",
      "Epoch 19/120\n",
      "10000/10000 [==============================] - 4s 425us/sample - loss: 2.2359 - acc: 0.5373\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.7453 - acc: 0.6311 - val_loss: 2.2372 - val_acc: 0.5373\n",
      "Epoch 20/120\n",
      "10000/10000 [==============================] - 4s 427us/sample - loss: 2.2686 - acc: 0.5343\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.7247 - acc: 0.6378 - val_loss: 2.2643 - val_acc: 0.5343\n",
      "Epoch 21/120\n",
      "10000/10000 [==============================] - 4s 426us/sample - loss: 2.2551 - acc: 0.5507\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.7114 - acc: 0.6442 - val_loss: 2.2526 - val_acc: 0.5507\n",
      "Epoch 22/120\n",
      "10000/10000 [==============================] - 4s 428us/sample - loss: 2.1987 - acc: 0.5523\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.6888 - acc: 0.6509 - val_loss: 2.2034 - val_acc: 0.5523\n",
      "Epoch 23/120\n",
      "10000/10000 [==============================] - 4s 429us/sample - loss: 2.2372 - acc: 0.5522\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.6780 - acc: 0.6532 - val_loss: 2.2351 - val_acc: 0.5522\n",
      "Epoch 24/120\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 2.3281 - acc: 0.5391\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.6437 - acc: 0.6671 - val_loss: 2.3229 - val_acc: 0.5391\n",
      "Epoch 25/120\n",
      "10000/10000 [==============================] - 4s 420us/sample - loss: 2.3785 - acc: 0.5242\n",
      "313/313 [==============================] - 55s 175ms/step - loss: 1.6452 - acc: 0.6679 - val_loss: 2.3791 - val_acc: 0.5242\n",
      "Epoch 26/120\n",
      "10000/10000 [==============================] - 4s 421us/sample - loss: 2.2219 - acc: 0.5641\n",
      "313/313 [==============================] - 55s 174ms/step - loss: 1.6286 - acc: 0.6740 - val_loss: 2.2206 - val_acc: 0.5641\n",
      "Epoch 27/120\n",
      "10000/10000 [==============================] - 4s 428us/sample - loss: 2.2138 - acc: 0.5611\n",
      "313/313 [==============================] - 55s 175ms/step - loss: 1.6221 - acc: 0.6771 - val_loss: 2.2160 - val_acc: 0.5611\n",
      "Epoch 28/120\n",
      "10000/10000 [==============================] - 4s 428us/sample - loss: 2.1849 - acc: 0.5651\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.6125 - acc: 0.6815 - val_loss: 2.1823 - val_acc: 0.5651\n",
      "Epoch 29/120\n",
      "10000/10000 [==============================] - 4s 422us/sample - loss: 2.3694 - acc: 0.5494\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.6014 - acc: 0.6849 - val_loss: 2.3673 - val_acc: 0.5494\n",
      "Epoch 30/120\n",
      "10000/10000 [==============================] - 4s 425us/sample - loss: 2.1511 - acc: 0.5798\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.5889 - acc: 0.6936 - val_loss: 2.1466 - val_acc: 0.5798\n",
      "Epoch 31/120\n",
      "10000/10000 [==============================] - 4s 429us/sample - loss: 2.2519 - acc: 0.5712\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 1.5853 - acc: 0.6920 - val_loss: 2.2538 - val_acc: 0.5712\n",
      "Epoch 32/120\n",
      "10000/10000 [==============================] - 4s 428us/sample - loss: 2.3445 - acc: 0.5504\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.5756 - acc: 0.6977 - val_loss: 2.3431 - val_acc: 0.5504\n",
      "Epoch 33/120\n",
      "10000/10000 [==============================] - 4s 429us/sample - loss: 2.2633 - acc: 0.5672\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.5552 - acc: 0.7051 - val_loss: 2.2565 - val_acc: 0.5672\n",
      "Epoch 34/120\n",
      "10000/10000 [==============================] - 4s 430us/sample - loss: 2.3207 - acc: 0.5677\n",
      "313/313 [==============================] - 56s 178ms/step - loss: 1.5670 - acc: 0.7045 - val_loss: 2.3175 - val_acc: 0.5677\n",
      "Epoch 35/120\n",
      "10000/10000 [==============================] - 4s 430us/sample - loss: 2.2833 - acc: 0.5697\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.5643 - acc: 0.7045 - val_loss: 2.2804 - val_acc: 0.5697\n",
      "Epoch 36/120\n",
      "10000/10000 [==============================] - 4s 425us/sample - loss: 2.2181 - acc: 0.5865\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
      "313/313 [==============================] - 65s 207ms/step - loss: 1.5512 - acc: 0.7063 - val_loss: 2.2152 - val_acc: 0.5865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/120\n",
      "10000/10000 [==============================] - 4s 427us/sample - loss: 1.9390 - acc: 0.6418\n",
      "313/313 [==============================] - 54s 174ms/step - loss: 1.2632 - acc: 0.7951 - val_loss: 1.9342 - val_acc: 0.6418\n",
      "Epoch 38/120\n",
      "10000/10000 [==============================] - 4s 431us/sample - loss: 1.9172 - acc: 0.6476\n",
      "313/313 [==============================] - 56s 180ms/step - loss: 1.1272 - acc: 0.8305 - val_loss: 1.9125 - val_acc: 0.6476\n",
      "Epoch 39/120\n",
      "10000/10000 [==============================] - 4s 428us/sample - loss: 1.9409 - acc: 0.6514\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 1.0703 - acc: 0.8433 - val_loss: 1.9371 - val_acc: 0.6514\n",
      "Epoch 40/120\n",
      "10000/10000 [==============================] - 4s 440us/sample - loss: 1.9123 - acc: 0.6528\n",
      "313/313 [==============================] - 56s 179ms/step - loss: 1.0212 - acc: 0.8548 - val_loss: 1.9085 - val_acc: 0.6528\n",
      "Epoch 41/120\n",
      "10000/10000 [==============================] - 4s 448us/sample - loss: 1.8932 - acc: 0.6604\n",
      "313/313 [==============================] - 56s 180ms/step - loss: 0.9851 - acc: 0.8613 - val_loss: 1.8909 - val_acc: 0.6604\n",
      "Epoch 42/120\n",
      "10000/10000 [==============================] - 4s 445us/sample - loss: 1.9474 - acc: 0.6478\n",
      "313/313 [==============================] - 56s 179ms/step - loss: 0.9540 - acc: 0.8668 - val_loss: 1.9455 - val_acc: 0.6478\n",
      "Epoch 43/120\n",
      "10000/10000 [==============================] - 4s 437us/sample - loss: 1.9822 - acc: 0.6483\n",
      "313/313 [==============================] - 56s 178ms/step - loss: 0.9232 - acc: 0.8750 - val_loss: 1.9784 - val_acc: 0.6483\n",
      "Epoch 44/120\n",
      "10000/10000 [==============================] - 4s 427us/sample - loss: 1.9807 - acc: 0.6456\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 0.9002 - acc: 0.8749 - val_loss: 1.9792 - val_acc: 0.6456\n",
      "Epoch 45/120\n",
      "10000/10000 [==============================] - 4s 425us/sample - loss: 1.9988 - acc: 0.6438\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 0.8756 - acc: 0.8819 - val_loss: 1.9961 - val_acc: 0.6438\n",
      "Epoch 46/120\n",
      "10000/10000 [==============================] - 4s 425us/sample - loss: 1.9776 - acc: 0.6486\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 0.8602 - acc: 0.8816 - val_loss: 1.9734 - val_acc: 0.6486\n",
      "Epoch 47/120\n",
      "10000/10000 [==============================] - 4s 422us/sample - loss: 2.0162 - acc: 0.6452\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 0.8352 - acc: 0.8885 - val_loss: 2.0127 - val_acc: 0.6452\n",
      "Epoch 48/120\n",
      "10000/10000 [==============================] - 4s 432us/sample - loss: 1.9122 - acc: 0.6611\n",
      "313/313 [==============================] - 56s 178ms/step - loss: 0.7703 - acc: 0.9094 - val_loss: 1.9095 - val_acc: 0.6611\n",
      "Epoch 49/120\n",
      "10000/10000 [==============================] - 4s 427us/sample - loss: 1.9216 - acc: 0.6613\n",
      "313/313 [==============================] - 56s 177ms/step - loss: 0.7374 - acc: 0.9216 - val_loss: 1.9184 - val_acc: 0.6613\n",
      "Epoch 50/120\n",
      "10000/10000 [==============================] - 4s 432us/sample - loss: 1.9383 - acc: 0.6620\n",
      "313/313 [==============================] - 56s 177ms/step - loss: 0.7227 - acc: 0.9252 - val_loss: 1.9347 - val_acc: 0.6620\n",
      "Epoch 51/120\n",
      "10000/10000 [==============================] - 4s 424us/sample - loss: 1.9254 - acc: 0.6648\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 0.7148 - acc: 0.9262 - val_loss: 1.9213 - val_acc: 0.6648\n",
      "Epoch 52/120\n",
      "10000/10000 [==============================] - 4s 424us/sample - loss: 1.9532 - acc: 0.6600\n",
      "313/313 [==============================] - 55s 175ms/step - loss: 0.7025 - acc: 0.9305 - val_loss: 1.9499 - val_acc: 0.6600\n",
      "Epoch 53/120\n",
      "10000/10000 [==============================] - 4s 427us/sample - loss: 1.9426 - acc: 0.6658\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 0.6989 - acc: 0.9313 - val_loss: 1.9381 - val_acc: 0.6658\n",
      "Epoch 54/120\n",
      "10000/10000 [==============================] - 4s 423us/sample - loss: 1.9388 - acc: 0.6648\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 0.6850 - acc: 0.9328 - val_loss: 1.9351 - val_acc: 0.6648\n"
     ]
    }
   ],
   "source": [
    "model = resnet.get_model()\n",
    "\n",
    "optimizer = SGD(lr=learning_rate, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "log= TensorBoard(log_dir=\"./logs/baseline_1\")\n",
    "\n",
    "callbacks = [log,EarlyStopping(monitor = 'val_loss', patience = 13), \n",
    "             ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 6, verbose = 1,min_lr=1e-5)]\n",
    "\n",
    "datagen = ImageDataGenerator(width_shift_range=0.15,\n",
    "                             height_shift_range=0.15,\n",
    "                             horizontal_flip=True)\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "          validation_data=(x_val, y_val),\n",
    "          steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model.save('model_baseline_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d3693f",
   "metadata": {},
   "source": [
    "### Cutout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e138ed0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "10000/10000 [==============================] - 7s 660us/sample - loss: 4.7118 - acc: 0.1047\n",
      "312/312 [==============================] - 61s 194ms/step - loss: 4.3307 - acc: 0.1061 - val_loss: 4.7192 - val_acc: 0.1047\n",
      "Epoch 2/120\n",
      "10000/10000 [==============================] - 4s 430us/sample - loss: 3.5576 - acc: 0.2192\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 3.7099 - acc: 0.1968 - val_loss: 3.5594 - val_acc: 0.2192\n",
      "Epoch 3/120\n",
      "10000/10000 [==============================] - 4s 422us/sample - loss: 3.1618 - acc: 0.3043\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 3.3554 - acc: 0.2609 - val_loss: 3.1621 - val_acc: 0.3043\n",
      "Epoch 4/120\n",
      "10000/10000 [==============================] - 4s 424us/sample - loss: 3.0353 - acc: 0.3283\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 3.1178 - acc: 0.3034 - val_loss: 3.0320 - val_acc: 0.3283\n",
      "Epoch 5/120\n",
      "10000/10000 [==============================] - 4s 440us/sample - loss: 2.9993 - acc: 0.3405\n",
      "312/312 [==============================] - 53s 171ms/step - loss: 2.9250 - acc: 0.3421 - val_loss: 2.9952 - val_acc: 0.3405\n",
      "Epoch 6/120\n",
      "10000/10000 [==============================] - 4s 428us/sample - loss: 2.9560 - acc: 0.3469\n",
      "312/312 [==============================] - 54s 172ms/step - loss: 2.7840 - acc: 0.3704 - val_loss: 2.9505 - val_acc: 0.3469\n",
      "Epoch 7/120\n",
      "10000/10000 [==============================] - 4s 438us/sample - loss: 2.7819 - acc: 0.3812\n",
      "312/312 [==============================] - 54s 174ms/step - loss: 2.6472 - acc: 0.3990 - val_loss: 2.7740 - val_acc: 0.3812\n",
      "Epoch 8/120\n",
      "10000/10000 [==============================] - 4s 432us/sample - loss: 2.4944 - acc: 0.4392\n",
      "312/312 [==============================] - 55s 177ms/step - loss: 2.5413 - acc: 0.4249 - val_loss: 2.4953 - val_acc: 0.4392\n",
      "Epoch 9/120\n",
      "10000/10000 [==============================] - 4s 429us/sample - loss: 2.7590 - acc: 0.3971\n",
      "312/312 [==============================] - 54s 175ms/step - loss: 2.4464 - acc: 0.4453 - val_loss: 2.7580 - val_acc: 0.3971\n",
      "Epoch 10/120\n",
      "10000/10000 [==============================] - 4s 433us/sample - loss: 2.5516 - acc: 0.4412\n",
      "312/312 [==============================] - 56s 180ms/step - loss: 2.3788 - acc: 0.4647 - val_loss: 2.5491 - val_acc: 0.4412\n",
      "Epoch 11/120\n",
      "10000/10000 [==============================] - 4s 431us/sample - loss: 2.4246 - acc: 0.4576\n",
      "312/312 [==============================] - 55s 176ms/step - loss: 2.3065 - acc: 0.4793 - val_loss: 2.4227 - val_acc: 0.4576\n",
      "Epoch 12/120\n",
      "10000/10000 [==============================] - 4s 431us/sample - loss: 2.4823 - acc: 0.4606\n",
      "312/312 [==============================] - 56s 178ms/step - loss: 2.2491 - acc: 0.4931 - val_loss: 2.4779 - val_acc: 0.4606\n",
      "Epoch 13/120\n",
      "10000/10000 [==============================] - 4s 431us/sample - loss: 2.4924 - acc: 0.4642\n",
      "312/312 [==============================] - 55s 175ms/step - loss: 2.1874 - acc: 0.5105 - val_loss: 2.4854 - val_acc: 0.4642\n",
      "Epoch 14/120\n",
      "10000/10000 [==============================] - 4s 431us/sample - loss: 2.3173 - acc: 0.5050\n",
      "312/312 [==============================] - 55s 175ms/step - loss: 2.1588 - acc: 0.5206 - val_loss: 2.3129 - val_acc: 0.5050\n",
      "Epoch 15/120\n",
      "10000/10000 [==============================] - 4s 433us/sample - loss: 2.4456 - acc: 0.4776\n",
      "312/312 [==============================] - 55s 175ms/step - loss: 2.1130 - acc: 0.5309 - val_loss: 2.4435 - val_acc: 0.4776\n",
      "Epoch 16/120\n",
      "10000/10000 [==============================] - 4s 429us/sample - loss: 2.2215 - acc: 0.5172\n",
      "312/312 [==============================] - 55s 175ms/step - loss: 2.0796 - acc: 0.5376 - val_loss: 2.2181 - val_acc: 0.5172\n",
      "Epoch 17/120\n",
      "10000/10000 [==============================] - 4s 430us/sample - loss: 2.2741 - acc: 0.5094\n",
      "312/312 [==============================] - 54s 174ms/step - loss: 2.0396 - acc: 0.5535 - val_loss: 2.2722 - val_acc: 0.5094\n",
      "Epoch 18/120\n",
      "10000/10000 [==============================] - 4s 429us/sample - loss: 2.2111 - acc: 0.5282\n",
      "312/312 [==============================] - 55s 175ms/step - loss: 2.0202 - acc: 0.5603 - val_loss: 2.2066 - val_acc: 0.5282\n",
      "Epoch 19/120\n",
      "10000/10000 [==============================] - 4s 435us/sample - loss: 2.2569 - acc: 0.5228\n",
      "312/312 [==============================] - 56s 178ms/step - loss: 1.9941 - acc: 0.5660 - val_loss: 2.2485 - val_acc: 0.5228\n",
      "Epoch 20/120\n",
      "10000/10000 [==============================] - 4s 428us/sample - loss: 2.3787 - acc: 0.5027\n",
      "312/312 [==============================] - 55s 176ms/step - loss: 1.9713 - acc: 0.5715 - val_loss: 2.3704 - val_acc: 0.5027\n",
      "Epoch 21/120\n",
      "10000/10000 [==============================] - 4s 435us/sample - loss: 2.1884 - acc: 0.5365\n",
      "312/312 [==============================] - 55s 176ms/step - loss: 1.9474 - acc: 0.5797 - val_loss: 2.1809 - val_acc: 0.5365\n",
      "Epoch 22/120\n",
      "10000/10000 [==============================] - 4s 436us/sample - loss: 2.2105 - acc: 0.5454\n",
      "312/312 [==============================] - 55s 176ms/step - loss: 1.9249 - acc: 0.5846 - val_loss: 2.2017 - val_acc: 0.5454\n",
      "Epoch 23/120\n",
      "10000/10000 [==============================] - 4s 435us/sample - loss: 2.4075 - acc: 0.5134\n",
      "312/312 [==============================] - 55s 176ms/step - loss: 1.9224 - acc: 0.5911 - val_loss: 2.3979 - val_acc: 0.5134\n",
      "Epoch 24/120\n",
      "10000/10000 [==============================] - 4s 435us/sample - loss: 2.2764 - acc: 0.5292\n",
      "312/312 [==============================] - 55s 176ms/step - loss: 1.8973 - acc: 0.5991 - val_loss: 2.2715 - val_acc: 0.5292\n",
      "Epoch 25/120\n",
      "10000/10000 [==============================] - 4s 435us/sample - loss: 2.1809 - acc: 0.5551\n",
      "312/312 [==============================] - 55s 176ms/step - loss: 1.8795 - acc: 0.6057 - val_loss: 2.1772 - val_acc: 0.5551\n",
      "Epoch 26/120\n",
      "10000/10000 [==============================] - 4s 443us/sample - loss: 2.2335 - acc: 0.5477\n",
      "312/312 [==============================] - 55s 177ms/step - loss: 1.8663 - acc: 0.6080 - val_loss: 2.2350 - val_acc: 0.5477\n",
      "Epoch 27/120\n",
      "10000/10000 [==============================] - 4s 447us/sample - loss: 2.1630 - acc: 0.5579\n",
      "312/312 [==============================] - 56s 181ms/step - loss: 1.8635 - acc: 0.6127 - val_loss: 2.1561 - val_acc: 0.5579\n",
      "Epoch 28/120\n",
      "10000/10000 [==============================] - 4s 438us/sample - loss: 2.1881 - acc: 0.5560\n",
      "312/312 [==============================] - 56s 180ms/step - loss: 1.8444 - acc: 0.6170 - val_loss: 2.1873 - val_acc: 0.5560\n",
      "Epoch 29/120\n",
      "10000/10000 [==============================] - 4s 432us/sample - loss: 2.1765 - acc: 0.5553\n",
      "312/312 [==============================] - 55s 177ms/step - loss: 1.8386 - acc: 0.6199 - val_loss: 2.1743 - val_acc: 0.5553\n",
      "Epoch 30/120\n",
      "10000/10000 [==============================] - 4s 431us/sample - loss: 2.2220 - acc: 0.5527\n",
      "312/312 [==============================] - 55s 176ms/step - loss: 1.8451 - acc: 0.6185 - val_loss: 2.2176 - val_acc: 0.5527\n",
      "Epoch 31/120\n",
      "10000/10000 [==============================] - 4s 438us/sample - loss: 2.2937 - acc: 0.5470\n",
      "312/312 [==============================] - 55s 177ms/step - loss: 1.8276 - acc: 0.6233 - val_loss: 2.2892 - val_acc: 0.5470\n",
      "Epoch 32/120\n",
      "10000/10000 [==============================] - 4s 440us/sample - loss: 2.2673 - acc: 0.5605\n",
      "312/312 [==============================] - 55s 178ms/step - loss: 1.8148 - acc: 0.6295 - val_loss: 2.2637 - val_acc: 0.5605\n",
      "Epoch 33/120\n",
      "10000/10000 [==============================] - 4s 440us/sample - loss: 2.2724 - acc: 0.5478\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
      "312/312 [==============================] - 71s 227ms/step - loss: 1.8130 - acc: 0.6337 - val_loss: 2.2700 - val_acc: 0.5478\n",
      "Epoch 34/120\n",
      "10000/10000 [==============================] - 4s 437us/sample - loss: 1.8311 - acc: 0.6412\n",
      "312/312 [==============================] - 54s 174ms/step - loss: 1.5352 - acc: 0.7066 - val_loss: 1.8262 - val_acc: 0.6412\n",
      "Epoch 35/120\n",
      "10000/10000 [==============================] - 4s 442us/sample - loss: 1.8138 - acc: 0.6442\n",
      "312/312 [==============================] - 55s 177ms/step - loss: 1.3989 - acc: 0.7418 - val_loss: 1.8083 - val_acc: 0.6442\n",
      "Epoch 36/120\n",
      "10000/10000 [==============================] - 5s 452us/sample - loss: 1.7889 - acc: 0.6511\n",
      "312/312 [==============================] - 56s 179ms/step - loss: 1.3477 - acc: 0.7528 - val_loss: 1.7832 - val_acc: 0.6511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/120\n",
      "10000/10000 [==============================] - 5s 462us/sample - loss: 1.7813 - acc: 0.6545\n",
      "312/312 [==============================] - 57s 181ms/step - loss: 1.3148 - acc: 0.7554 - val_loss: 1.7783 - val_acc: 0.6545\n",
      "Epoch 38/120\n",
      "10000/10000 [==============================] - 5s 468us/sample - loss: 1.7930 - acc: 0.6501\n",
      "312/312 [==============================] - 58s 184ms/step - loss: 1.2782 - acc: 0.7645 - val_loss: 1.7881 - val_acc: 0.6501\n",
      "Epoch 39/120\n",
      "10000/10000 [==============================] - 5s 474us/sample - loss: 1.7666 - acc: 0.6550\n",
      "312/312 [==============================] - 59s 188ms/step - loss: 1.2559 - acc: 0.7686 - val_loss: 1.7600 - val_acc: 0.6550\n",
      "Epoch 40/120\n",
      "10000/10000 [==============================] - 5s 468us/sample - loss: 1.7789 - acc: 0.6530\n",
      "312/312 [==============================] - 60s 193ms/step - loss: 1.2218 - acc: 0.7738 - val_loss: 1.7770 - val_acc: 0.6530\n",
      "Epoch 41/120\n",
      "10000/10000 [==============================] - 5s 479us/sample - loss: 1.8052 - acc: 0.6458\n",
      "312/312 [==============================] - 58s 186ms/step - loss: 1.2076 - acc: 0.7764 - val_loss: 1.8025 - val_acc: 0.6458\n",
      "Epoch 42/120\n",
      "10000/10000 [==============================] - 5s 483us/sample - loss: 1.8024 - acc: 0.6471\n",
      "312/312 [==============================] - 58s 187ms/step - loss: 1.1926 - acc: 0.7781 - val_loss: 1.7996 - val_acc: 0.6471\n",
      "Epoch 43/120\n",
      "10000/10000 [==============================] - 5s 478us/sample - loss: 1.7750 - acc: 0.6547\n",
      "312/312 [==============================] - 58s 186ms/step - loss: 1.1720 - acc: 0.7842 - val_loss: 1.7712 - val_acc: 0.6547\n",
      "Epoch 44/120\n",
      "10000/10000 [==============================] - 5s 491us/sample - loss: 1.7832 - acc: 0.6493\n",
      "312/312 [==============================] - 59s 189ms/step - loss: 1.1514 - acc: 0.7842 - val_loss: 1.7770 - val_acc: 0.6493\n",
      "Epoch 45/120\n",
      "10000/10000 [==============================] - 5s 483us/sample - loss: 1.8333 - acc: 0.6394\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
      "312/312 [==============================] - 59s 188ms/step - loss: 1.1328 - acc: 0.7906 - val_loss: 1.8278 - val_acc: 0.6394\n",
      "Epoch 46/120\n",
      "10000/10000 [==============================] - 5s 470us/sample - loss: 1.7301 - acc: 0.6633\n",
      "312/312 [==============================] - 59s 188ms/step - loss: 1.0464 - acc: 0.8142 - val_loss: 1.7247 - val_acc: 0.6633\n",
      "Epoch 47/120\n",
      "10000/10000 [==============================] - 5s 475us/sample - loss: 1.7535 - acc: 0.6621\n",
      "312/312 [==============================] - 59s 188ms/step - loss: 1.0079 - acc: 0.8263 - val_loss: 1.7484 - val_acc: 0.6621\n",
      "Epoch 48/120\n",
      "10000/10000 [==============================] - 5s 490us/sample - loss: 1.7452 - acc: 0.6647\n",
      "312/312 [==============================] - 58s 187ms/step - loss: 0.9999 - acc: 0.8260 - val_loss: 1.7400 - val_acc: 0.6647\n",
      "Epoch 49/120\n",
      "10000/10000 [==============================] - 5s 473us/sample - loss: 1.7275 - acc: 0.6654\n",
      "312/312 [==============================] - 58s 187ms/step - loss: 0.9864 - acc: 0.8289 - val_loss: 1.7228 - val_acc: 0.6654\n",
      "Epoch 50/120\n",
      "10000/10000 [==============================] - 5s 475us/sample - loss: 1.7252 - acc: 0.6686\n",
      "312/312 [==============================] - 59s 188ms/step - loss: 0.9722 - acc: 0.8353 - val_loss: 1.7197 - val_acc: 0.6686\n",
      "Epoch 51/120\n",
      "10000/10000 [==============================] - 5s 475us/sample - loss: 1.7480 - acc: 0.6593\n",
      "312/312 [==============================] - 59s 188ms/step - loss: 0.9682 - acc: 0.8335 - val_loss: 1.7432 - val_acc: 0.6593\n",
      "Epoch 52/120\n",
      "10000/10000 [==============================] - 5s 486us/sample - loss: 1.7373 - acc: 0.6608\n",
      "312/312 [==============================] - 59s 188ms/step - loss: 0.9629 - acc: 0.8346 - val_loss: 1.7313 - val_acc: 0.6608\n",
      "Epoch 53/120\n",
      "10000/10000 [==============================] - 5s 484us/sample - loss: 1.7300 - acc: 0.6642\n",
      "312/312 [==============================] - 58s 185ms/step - loss: 0.9432 - acc: 0.8403 - val_loss: 1.7243 - val_acc: 0.6642\n",
      "Epoch 54/120\n",
      "10000/10000 [==============================] - 5s 480us/sample - loss: 1.7301 - acc: 0.6674\n",
      "312/312 [==============================] - 58s 186ms/step - loss: 0.9428 - acc: 0.8402 - val_loss: 1.7243 - val_acc: 0.6674\n",
      "Epoch 55/120\n",
      "10000/10000 [==============================] - 5s 469us/sample - loss: 1.7538 - acc: 0.6608\n",
      "312/312 [==============================] - 58s 185ms/step - loss: 0.9443 - acc: 0.8393 - val_loss: 1.7464 - val_acc: 0.6608\n",
      "Epoch 56/120\n",
      "10000/10000 [==============================] - 5s 477us/sample - loss: 1.7443 - acc: 0.6633\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
      "312/312 [==============================] - 58s 187ms/step - loss: 0.9317 - acc: 0.8424 - val_loss: 1.7385 - val_acc: 0.6633\n",
      "Epoch 57/120\n",
      "10000/10000 [==============================] - 5s 481us/sample - loss: 1.7336 - acc: 0.6645\n",
      "312/312 [==============================] - 58s 185ms/step - loss: 0.9181 - acc: 0.8457 - val_loss: 1.7270 - val_acc: 0.6645\n",
      "Epoch 58/120\n",
      "10000/10000 [==============================] - 5s 472us/sample - loss: 1.7306 - acc: 0.6669\n",
      "312/312 [==============================] - 58s 187ms/step - loss: 0.9036 - acc: 0.8502 - val_loss: 1.7239 - val_acc: 0.6669\n",
      "Epoch 59/120\n",
      "10000/10000 [==============================] - 5s 470us/sample - loss: 1.7349 - acc: 0.6674\n",
      "312/312 [==============================] - 58s 186ms/step - loss: 0.9074 - acc: 0.8501 - val_loss: 1.7280 - val_acc: 0.6674\n",
      "Epoch 60/120\n",
      "10000/10000 [==============================] - 5s 467us/sample - loss: 1.7331 - acc: 0.6683\n",
      "312/312 [==============================] - 58s 186ms/step - loss: 0.9056 - acc: 0.8507 - val_loss: 1.7264 - val_acc: 0.6683\n",
      "Epoch 61/120\n",
      "10000/10000 [==============================] - 5s 460us/sample - loss: 1.7269 - acc: 0.6672\n",
      "312/312 [==============================] - 57s 183ms/step - loss: 0.8982 - acc: 0.8514 - val_loss: 1.7199 - val_acc: 0.6672\n",
      "Epoch 62/120\n",
      "10000/10000 [==============================] - 5s 461us/sample - loss: 1.7271 - acc: 0.6669\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
      "312/312 [==============================] - 57s 182ms/step - loss: 0.9013 - acc: 0.8526 - val_loss: 1.7202 - val_acc: 0.6669\n",
      "Epoch 63/120\n",
      "10000/10000 [==============================] - 5s 455us/sample - loss: 1.7282 - acc: 0.6676\n",
      "312/312 [==============================] - 57s 182ms/step - loss: 0.8881 - acc: 0.8543 - val_loss: 1.7212 - val_acc: 0.6676\n"
     ]
    }
   ],
   "source": [
    "model = resnet.get_model()\n",
    "optimizer = SGD(lr=learning_rate, momentum=0.9)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "log= TensorBoard(log_dir=\"./logs/cutout\")\n",
    "\n",
    "callbacks = [log,EarlyStopping(monitor = 'val_loss', patience = 13), \n",
    "             ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 6, verbose = 1,min_lr=1e-4)]\n",
    "\n",
    "datagen = CutoutImageDataGenerator(width_shift_range=0.15,\n",
    "                                   height_shift_range=0.15,\n",
    "                                   horizontal_flip=True,\n",
    "                                   cutout_mask_size=10)\n",
    "\n",
    "model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "          validation_data=(x_val, y_val),\n",
    "          steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model.save('model_cutout.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9db4087",
   "metadata": {},
   "source": [
    "### Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf47a70b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "10000/10000 [==============================] - 4s 370us/sample - loss: 4.7407 - acc: 0.0910\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 4.5433 - acc: 0.0933 - val_loss: 4.7459 - val_acc: 0.0910\n",
      "Epoch 2/120\n",
      "10000/10000 [==============================] - 3s 316us/sample - loss: 3.6656 - acc: 0.2093\n",
      "312/312 [==============================] - 49s 155ms/step - loss: 4.0656 - acc: 0.1755 - val_loss: 3.6674 - val_acc: 0.2093\n",
      "Epoch 3/120\n",
      "10000/10000 [==============================] - 3s 318us/sample - loss: 3.4119 - acc: 0.2510\n",
      "312/312 [==============================] - 50s 160ms/step - loss: 3.7988 - acc: 0.2280 - val_loss: 3.4070 - val_acc: 0.2510\n",
      "Epoch 4/120\n",
      "10000/10000 [==============================] - 3s 321us/sample - loss: 3.1524 - acc: 0.2998\n",
      "312/312 [==============================] - 50s 161ms/step - loss: 3.5955 - acc: 0.2789 - val_loss: 3.1434 - val_acc: 0.2998\n",
      "Epoch 5/120\n",
      "10000/10000 [==============================] - 3s 319us/sample - loss: 2.8746 - acc: 0.3520\n",
      "312/312 [==============================] - 50s 161ms/step - loss: 3.4305 - acc: 0.3100 - val_loss: 2.8690 - val_acc: 0.3520\n",
      "Epoch 6/120\n",
      "10000/10000 [==============================] - 3s 331us/sample - loss: 2.7341 - acc: 0.3791\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 3.2972 - acc: 0.3445 - val_loss: 2.7320 - val_acc: 0.3791\n",
      "Epoch 7/120\n",
      "10000/10000 [==============================] - 3s 323us/sample - loss: 2.6876 - acc: 0.3905\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 3.1946 - acc: 0.3711 - val_loss: 2.6875 - val_acc: 0.3905\n",
      "Epoch 8/120\n",
      "10000/10000 [==============================] - 3s 329us/sample - loss: 2.5172 - acc: 0.4267\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 3.1006 - acc: 0.3942 - val_loss: 2.5146 - val_acc: 0.4267\n",
      "Epoch 9/120\n",
      "10000/10000 [==============================] - 3s 325us/sample - loss: 2.5913 - acc: 0.4158\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 3.0261 - acc: 0.4195 - val_loss: 2.5855 - val_acc: 0.4158\n",
      "Epoch 10/120\n",
      "10000/10000 [==============================] - 3s 329us/sample - loss: 2.3666 - acc: 0.4571\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.9517 - acc: 0.4383 - val_loss: 2.3619 - val_acc: 0.4571\n",
      "Epoch 11/120\n",
      "10000/10000 [==============================] - 3s 338us/sample - loss: 2.3246 - acc: 0.4720\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.9066 - acc: 0.4531 - val_loss: 2.3219 - val_acc: 0.4720\n",
      "Epoch 12/120\n",
      "10000/10000 [==============================] - 3s 337us/sample - loss: 2.2481 - acc: 0.4846\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.8565 - acc: 0.4613 - val_loss: 2.2408 - val_acc: 0.4846\n",
      "Epoch 13/120\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 2.2286 - acc: 0.4807\n",
      "312/312 [==============================] - 52s 165ms/step - loss: 2.8104 - acc: 0.4778 - val_loss: 2.2213 - val_acc: 0.4807\n",
      "Epoch 14/120\n",
      "10000/10000 [==============================] - 3s 333us/sample - loss: 2.1222 - acc: 0.5133\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.7793 - acc: 0.4857 - val_loss: 2.1140 - val_acc: 0.5133\n",
      "Epoch 15/120\n",
      "10000/10000 [==============================] - 3s 347us/sample - loss: 2.1631 - acc: 0.5104\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 2.7492 - acc: 0.4979 - val_loss: 2.1558 - val_acc: 0.5104\n",
      "Epoch 16/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 2.2075 - acc: 0.5004\n",
      "312/312 [==============================] - 53s 171ms/step - loss: 2.7196 - acc: 0.5086 - val_loss: 2.2025 - val_acc: 0.5004\n",
      "Epoch 17/120\n",
      "10000/10000 [==============================] - 3s 337us/sample - loss: 2.1386 - acc: 0.5179\n",
      "312/312 [==============================] - 54s 174ms/step - loss: 2.6871 - acc: 0.5166 - val_loss: 2.1293 - val_acc: 0.5179\n",
      "Epoch 18/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 2.1759 - acc: 0.5082\n",
      "312/312 [==============================] - 52s 165ms/step - loss: 2.6746 - acc: 0.5233 - val_loss: 2.1655 - val_acc: 0.5082\n",
      "Epoch 19/120\n",
      "10000/10000 [==============================] - 3s 333us/sample - loss: 2.0861 - acc: 0.5367\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.6433 - acc: 0.5320 - val_loss: 2.0782 - val_acc: 0.5367\n",
      "Epoch 20/120\n",
      "10000/10000 [==============================] - 3s 331us/sample - loss: 2.1192 - acc: 0.5243\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.6264 - acc: 0.5380 - val_loss: 2.1132 - val_acc: 0.5243\n",
      "Epoch 21/120\n",
      "10000/10000 [==============================] - 3s 331us/sample - loss: 2.0545 - acc: 0.5406\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.6202 - acc: 0.5414 - val_loss: 2.0483 - val_acc: 0.5406\n",
      "Epoch 22/120\n",
      "10000/10000 [==============================] - 3s 338us/sample - loss: 2.0244 - acc: 0.5476\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.5921 - acc: 0.5512 - val_loss: 2.0167 - val_acc: 0.5476\n",
      "Epoch 23/120\n",
      "10000/10000 [==============================] - 3s 333us/sample - loss: 2.0190 - acc: 0.5473\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.5737 - acc: 0.5566 - val_loss: 2.0109 - val_acc: 0.5473\n",
      "Epoch 24/120\n",
      "10000/10000 [==============================] - 3s 330us/sample - loss: 1.9561 - acc: 0.5635\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.5655 - acc: 0.5612 - val_loss: 1.9461 - val_acc: 0.5635\n",
      "Epoch 25/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 2.0406 - acc: 0.5435\n",
      "312/312 [==============================] - 52s 165ms/step - loss: 2.5670 - acc: 0.5657 - val_loss: 2.0365 - val_acc: 0.5435\n",
      "Epoch 26/120\n",
      "10000/10000 [==============================] - 3s 337us/sample - loss: 1.9878 - acc: 0.5661\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.5500 - acc: 0.5689 - val_loss: 1.9805 - val_acc: 0.5661\n",
      "Epoch 27/120\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 1.9824 - acc: 0.5615\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.5499 - acc: 0.5694 - val_loss: 1.9737 - val_acc: 0.5615\n",
      "Epoch 28/120\n",
      "10000/10000 [==============================] - 6s 550us/sample - loss: 1.9495 - acc: 0.5705\n",
      "312/312 [==============================] - 54s 172ms/step - loss: 2.5280 - acc: 0.5754 - val_loss: 1.9413 - val_acc: 0.5705\n",
      "Epoch 29/120\n",
      "10000/10000 [==============================] - 3s 342us/sample - loss: 1.9673 - acc: 0.5666\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.5277 - acc: 0.5780 - val_loss: 1.9585 - val_acc: 0.5666\n",
      "Epoch 30/120\n",
      "10000/10000 [==============================] - 3s 343us/sample - loss: 1.9518 - acc: 0.5662\n",
      "312/312 [==============================] - 52s 168ms/step - loss: 2.5081 - acc: 0.5864 - val_loss: 1.9464 - val_acc: 0.5662\n",
      "Epoch 31/120\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 1.9567 - acc: 0.5790\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 2.4956 - acc: 0.5870 - val_loss: 1.9458 - val_acc: 0.5790\n",
      "Epoch 32/120\n",
      "10000/10000 [==============================] - 3s 337us/sample - loss: 2.0081 - acc: 0.5655\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.4893 - acc: 0.5930 - val_loss: 2.0028 - val_acc: 0.5655\n",
      "Epoch 33/120\n",
      "10000/10000 [==============================] - 3s 333us/sample - loss: 1.9698 - acc: 0.5733\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.4871 - acc: 0.5928 - val_loss: 1.9608 - val_acc: 0.5733\n",
      "Epoch 34/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 1.9260 - acc: 0.5779\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.4815 - acc: 0.5976 - val_loss: 1.9163 - val_acc: 0.5779\n",
      "Epoch 35/120\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 1.9899 - acc: 0.5655\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.4784 - acc: 0.5980 - val_loss: 1.9808 - val_acc: 0.5655\n",
      "Epoch 36/120\n",
      "10000/10000 [==============================] - 3s 344us/sample - loss: 1.8851 - acc: 0.5914\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 2.4704 - acc: 0.5996 - val_loss: 1.8741 - val_acc: 0.5914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 1.9660 - acc: 0.5708\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.4710 - acc: 0.6023 - val_loss: 1.9589 - val_acc: 0.5708\n",
      "Epoch 38/120\n",
      "10000/10000 [==============================] - 3s 327us/sample - loss: 1.9374 - acc: 0.5810\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.4559 - acc: 0.6057 - val_loss: 1.9265 - val_acc: 0.5810\n",
      "Epoch 39/120\n",
      "10000/10000 [==============================] - 3s 331us/sample - loss: 1.9474 - acc: 0.5772\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.4564 - acc: 0.6089 - val_loss: 1.9374 - val_acc: 0.5772\n",
      "Epoch 40/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 1.9582 - acc: 0.5753\n",
      "312/312 [==============================] - 52s 165ms/step - loss: 2.4342 - acc: 0.6131 - val_loss: 1.9513 - val_acc: 0.5753\n",
      "Epoch 41/120\n",
      "10000/10000 [==============================] - 3s 329us/sample - loss: 2.0252 - acc: 0.5634\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.4444 - acc: 0.6131 - val_loss: 2.0188 - val_acc: 0.5634\n",
      "Epoch 42/120\n",
      "10000/10000 [==============================] - 3s 325us/sample - loss: 1.9948 - acc: 0.5680\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.030000000447034835.\n",
      "312/312 [==============================] - 55s 176ms/step - loss: 2.4392 - acc: 0.6155 - val_loss: 1.9880 - val_acc: 0.5680\n",
      "Epoch 43/120\n",
      "10000/10000 [==============================] - 3s 323us/sample - loss: 1.6500 - acc: 0.6501\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.2727 - acc: 0.6718 - val_loss: 1.6424 - val_acc: 0.6501\n",
      "Epoch 44/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 1.6200 - acc: 0.6536\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.1877 - acc: 0.6990 - val_loss: 1.6111 - val_acc: 0.6536\n",
      "Epoch 45/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 1.6335 - acc: 0.6543\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.1621 - acc: 0.7036 - val_loss: 1.6240 - val_acc: 0.6543\n",
      "Epoch 46/120\n",
      "10000/10000 [==============================] - 3s 333us/sample - loss: 1.6428 - acc: 0.6525\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.1343 - acc: 0.7091 - val_loss: 1.6336 - val_acc: 0.6525\n",
      "Epoch 47/120\n",
      "10000/10000 [==============================] - 3s 341us/sample - loss: 1.6150 - acc: 0.6529\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 2.1053 - acc: 0.7161 - val_loss: 1.6072 - val_acc: 0.6529\n",
      "Epoch 48/120\n",
      "10000/10000 [==============================] - 3s 331us/sample - loss: 1.6246 - acc: 0.6511\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.1136 - acc: 0.7142 - val_loss: 1.6142 - val_acc: 0.6511\n",
      "Epoch 49/120\n",
      "10000/10000 [==============================] - 3s 327us/sample - loss: 1.6147 - acc: 0.6530\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.0780 - acc: 0.7215 - val_loss: 1.6060 - val_acc: 0.6530\n",
      "Epoch 50/120\n",
      "10000/10000 [==============================] - 3s 326us/sample - loss: 1.5990 - acc: 0.6552\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.0638 - acc: 0.7212 - val_loss: 1.5919 - val_acc: 0.6552\n",
      "Epoch 51/120\n",
      "10000/10000 [==============================] - 3s 326us/sample - loss: 1.6005 - acc: 0.6562\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.0631 - acc: 0.7181 - val_loss: 1.5915 - val_acc: 0.6562\n",
      "Epoch 52/120\n",
      "10000/10000 [==============================] - 3s 323us/sample - loss: 1.5909 - acc: 0.6534\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.0574 - acc: 0.7216 - val_loss: 1.5829 - val_acc: 0.6534\n",
      "Epoch 53/120\n",
      "10000/10000 [==============================] - 3s 324us/sample - loss: 1.6595 - acc: 0.6370\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.0373 - acc: 0.7261 - val_loss: 1.6514 - val_acc: 0.6370\n",
      "Epoch 54/120\n",
      "10000/10000 [==============================] - 3s 325us/sample - loss: 1.6575 - acc: 0.6404\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.0442 - acc: 0.7212 - val_loss: 1.6487 - val_acc: 0.6404\n",
      "Epoch 55/120\n",
      "10000/10000 [==============================] - 3s 325us/sample - loss: 1.5792 - acc: 0.6497\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.0476 - acc: 0.7197 - val_loss: 1.5698 - val_acc: 0.6497\n",
      "Epoch 56/120\n",
      "10000/10000 [==============================] - 3s 323us/sample - loss: 1.6025 - acc: 0.6509\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.0284 - acc: 0.7259 - val_loss: 1.5926 - val_acc: 0.6509\n",
      "Epoch 57/120\n",
      "10000/10000 [==============================] - 3s 326us/sample - loss: 1.6210 - acc: 0.6437\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.0235 - acc: 0.7274 - val_loss: 1.6105 - val_acc: 0.6437\n",
      "Epoch 58/120\n",
      "10000/10000 [==============================] - 3s 341us/sample - loss: 1.6019 - acc: 0.6506\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.0135 - acc: 0.7286 - val_loss: 1.5940 - val_acc: 0.6506\n",
      "Epoch 59/120\n",
      "10000/10000 [==============================] - 3s 342us/sample - loss: 1.6221 - acc: 0.6485\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 2.0172 - acc: 0.7294 - val_loss: 1.6113 - val_acc: 0.6485\n",
      "Epoch 60/120\n",
      "10000/10000 [==============================] - 3s 327us/sample - loss: 1.6081 - acc: 0.6504\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.0038 - acc: 0.7290 - val_loss: 1.5986 - val_acc: 0.6504\n",
      "Epoch 61/120\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 1.5897 - acc: 0.6467\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.009000000357627868.\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.0075 - acc: 0.7325 - val_loss: 1.5810 - val_acc: 0.6467\n",
      "Epoch 62/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 1.4939 - acc: 0.6751\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 1.9195 - acc: 0.7636 - val_loss: 1.4865 - val_acc: 0.6751\n",
      "Epoch 63/120\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 1.4877 - acc: 0.6736\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 1.8769 - acc: 0.7769 - val_loss: 1.4804 - val_acc: 0.6736\n",
      "Epoch 64/120\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 1.4984 - acc: 0.6735\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 1.8773 - acc: 0.7774 - val_loss: 1.4906 - val_acc: 0.6735\n",
      "Epoch 65/120\n",
      "10000/10000 [==============================] - 3s 334us/sample - loss: 1.4836 - acc: 0.6788\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 1.8642 - acc: 0.7799 - val_loss: 1.4761 - val_acc: 0.6788\n",
      "Epoch 66/120\n",
      "10000/10000 [==============================] - 3s 329us/sample - loss: 1.4785 - acc: 0.6739\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8513 - acc: 0.7822 - val_loss: 1.4705 - val_acc: 0.6739\n",
      "Epoch 67/120\n",
      "10000/10000 [==============================] - 3s 337us/sample - loss: 1.4881 - acc: 0.6717\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8371 - acc: 0.7860 - val_loss: 1.4813 - val_acc: 0.6717\n",
      "Epoch 68/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 1.4847 - acc: 0.6751\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 1.8367 - acc: 0.7890 - val_loss: 1.4762 - val_acc: 0.6751\n",
      "Epoch 69/120\n",
      "10000/10000 [==============================] - 3s 337us/sample - loss: 1.4694 - acc: 0.6774\n",
      "312/312 [==============================] - 52s 165ms/step - loss: 1.8327 - acc: 0.7890 - val_loss: 1.4599 - val_acc: 0.6774\n",
      "Epoch 70/120\n",
      "10000/10000 [==============================] - 3s 343us/sample - loss: 1.4815 - acc: 0.6758\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 1.8292 - acc: 0.7876 - val_loss: 1.4732 - val_acc: 0.6758\n",
      "Epoch 71/120\n",
      "10000/10000 [==============================] - 3s 341us/sample - loss: 1.4878 - acc: 0.6723\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 1.8116 - acc: 0.7921 - val_loss: 1.4794 - val_acc: 0.6723\n",
      "Epoch 72/120\n",
      "10000/10000 [==============================] - 3s 341us/sample - loss: 1.4871 - acc: 0.6763\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 1.8152 - acc: 0.7916 - val_loss: 1.4795 - val_acc: 0.6763\n",
      "Epoch 73/120\n",
      "10000/10000 [==============================] - 3s 334us/sample - loss: 1.4757 - acc: 0.6749\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 1.8023 - acc: 0.7948 - val_loss: 1.4671 - val_acc: 0.6749\n",
      "Epoch 74/120\n",
      "10000/10000 [==============================] - 3s 331us/sample - loss: 1.4729 - acc: 0.6754\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 1.8054 - acc: 0.7952 - val_loss: 1.4654 - val_acc: 0.6754\n",
      "Epoch 75/120\n",
      "10000/10000 [==============================] - 3s 328us/sample - loss: 1.4758 - acc: 0.6765\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 0.002700000163167715.\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 1.8031 - acc: 0.7935 - val_loss: 1.4672 - val_acc: 0.6765\n",
      "Epoch 76/120\n",
      "10000/10000 [==============================] - 3s 329us/sample - loss: 1.4546 - acc: 0.6802\n",
      "312/312 [==============================] - 52s 165ms/step - loss: 1.7704 - acc: 0.8048 - val_loss: 1.4458 - val_acc: 0.6802\n",
      "Epoch 77/120\n",
      "10000/10000 [==============================] - 3s 328us/sample - loss: 1.4525 - acc: 0.6791\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 1.7636 - acc: 0.8077 - val_loss: 1.4437 - val_acc: 0.6791\n",
      "Epoch 78/120\n",
      "10000/10000 [==============================] - 3s 324us/sample - loss: 1.4497 - acc: 0.6806\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.7444 - acc: 0.8111 - val_loss: 1.4413 - val_acc: 0.6806\n",
      "Epoch 79/120\n",
      "10000/10000 [==============================] - 3s 324us/sample - loss: 1.4566 - acc: 0.6806\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.7451 - acc: 0.8106 - val_loss: 1.4489 - val_acc: 0.6806\n",
      "Epoch 80/120\n",
      "10000/10000 [==============================] - 3s 333us/sample - loss: 1.4585 - acc: 0.6790\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 1.7535 - acc: 0.8129 - val_loss: 1.4511 - val_acc: 0.6790\n",
      "Epoch 81/120\n",
      "10000/10000 [==============================] - 3s 329us/sample - loss: 1.4456 - acc: 0.6834\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 1.7426 - acc: 0.8123 - val_loss: 1.4372 - val_acc: 0.6834\n",
      "Epoch 82/120\n",
      "10000/10000 [==============================] - 3s 325us/sample - loss: 1.4428 - acc: 0.6850\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 1.7428 - acc: 0.8126 - val_loss: 1.4348 - val_acc: 0.6850\n",
      "Epoch 83/120\n",
      "10000/10000 [==============================] - 3s 337us/sample - loss: 1.4464 - acc: 0.6823\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 1.7388 - acc: 0.8143 - val_loss: 1.4389 - val_acc: 0.6823\n",
      "Epoch 84/120\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 1.4531 - acc: 0.6796\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 1.7394 - acc: 0.8147 - val_loss: 1.4452 - val_acc: 0.6796\n",
      "Epoch 85/120\n",
      "10000/10000 [==============================] - 3s 344us/sample - loss: 1.4498 - acc: 0.6804\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 1.7346 - acc: 0.8165 - val_loss: 1.4412 - val_acc: 0.6804\n",
      "Epoch 86/120\n",
      "10000/10000 [==============================] - 3s 336us/sample - loss: 1.4550 - acc: 0.6796\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 1.7374 - acc: 0.8141 - val_loss: 1.4471 - val_acc: 0.6796\n",
      "Epoch 87/120\n",
      "10000/10000 [==============================] - 3s 345us/sample - loss: 1.4486 - acc: 0.6790\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 1.7243 - acc: 0.8191 - val_loss: 1.4404 - val_acc: 0.6790\n",
      "Epoch 88/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 1.4485 - acc: 0.6782\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 0.0008100000210106373.\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 1.7393 - acc: 0.8123 - val_loss: 1.4408 - val_acc: 0.6782\n",
      "Epoch 89/120\n",
      "10000/10000 [==============================] - 3s 341us/sample - loss: 1.4426 - acc: 0.6795\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 1.7222 - acc: 0.8217 - val_loss: 1.4349 - val_acc: 0.6795\n",
      "Epoch 90/120\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 1.4416 - acc: 0.6811\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 1.7206 - acc: 0.8202 - val_loss: 1.4343 - val_acc: 0.6811\n",
      "Epoch 91/120\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 1.4402 - acc: 0.6813\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 1.7189 - acc: 0.8198 - val_loss: 1.4326 - val_acc: 0.6813\n",
      "Epoch 92/120\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 1.4419 - acc: 0.6792\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 1.7140 - acc: 0.8232 - val_loss: 1.4340 - val_acc: 0.6792\n",
      "Epoch 93/120\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 1.4407 - acc: 0.6809\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 1.7140 - acc: 0.8226 - val_loss: 1.4329 - val_acc: 0.6809\n",
      "Epoch 94/120\n",
      "10000/10000 [==============================] - 3s 341us/sample - loss: 1.4382 - acc: 0.6820\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 1.7103 - acc: 0.8220 - val_loss: 1.4307 - val_acc: 0.6820\n",
      "Epoch 95/120\n",
      "10000/10000 [==============================] - 3s 327us/sample - loss: 1.4407 - acc: 0.6824\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 1.7165 - acc: 0.8226 - val_loss: 1.4335 - val_acc: 0.6824\n",
      "Epoch 96/120\n",
      "10000/10000 [==============================] - 3s 334us/sample - loss: 1.4383 - acc: 0.6819\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 1.7045 - acc: 0.8200 - val_loss: 1.4309 - val_acc: 0.6819\n",
      "Epoch 97/120\n",
      "10000/10000 [==============================] - 3s 329us/sample - loss: 1.4392 - acc: 0.6809\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.7136 - acc: 0.8216 - val_loss: 1.4316 - val_acc: 0.6809\n",
      "Epoch 98/120\n",
      "10000/10000 [==============================] - 3s 328us/sample - loss: 1.4395 - acc: 0.6800\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.7179 - acc: 0.8217 - val_loss: 1.4320 - val_acc: 0.6800\n",
      "Epoch 99/120\n",
      "10000/10000 [==============================] - 3s 327us/sample - loss: 1.4399 - acc: 0.6821\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.7029 - acc: 0.8232 - val_loss: 1.4325 - val_acc: 0.6821\n",
      "Epoch 100/120\n",
      "10000/10000 [==============================] - 3s 326us/sample - loss: 1.4372 - acc: 0.6806\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.7036 - acc: 0.8253 - val_loss: 1.4296 - val_acc: 0.6806\n",
      "Epoch 101/120\n",
      "10000/10000 [==============================] - 3s 325us/sample - loss: 1.4390 - acc: 0.6818\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.7105 - acc: 0.8192 - val_loss: 1.4319 - val_acc: 0.6818\n",
      "Epoch 102/120\n",
      "10000/10000 [==============================] - 3s 324us/sample - loss: 1.4367 - acc: 0.6807\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.7073 - acc: 0.8232 - val_loss: 1.4292 - val_acc: 0.6807\n",
      "Epoch 103/120\n",
      "10000/10000 [==============================] - 3s 323us/sample - loss: 1.4403 - acc: 0.6812\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 1.7068 - acc: 0.8208 - val_loss: 1.4329 - val_acc: 0.6812\n",
      "Epoch 104/120\n",
      "10000/10000 [==============================] - 3s 322us/sample - loss: 1.4410 - acc: 0.6813\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 1.7109 - acc: 0.8218 - val_loss: 1.4335 - val_acc: 0.6813\n",
      "Epoch 105/120\n",
      "10000/10000 [==============================] - 3s 324us/sample - loss: 1.4374 - acc: 0.6838\n",
      "312/312 [==============================] - 51s 162ms/step - loss: 1.7082 - acc: 0.8221 - val_loss: 1.4297 - val_acc: 0.6838\n",
      "Epoch 106/120\n",
      "10000/10000 [==============================] - 3s 321us/sample - loss: 1.4384 - acc: 0.6828\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 1.7084 - acc: 0.8229 - val_loss: 1.4311 - val_acc: 0.6828\n",
      "Epoch 107/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 331us/sample - loss: 1.4408 - acc: 0.6841\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 1.7003 - acc: 0.8234 - val_loss: 1.4326 - val_acc: 0.6841\n",
      "Epoch 108/120\n",
      "10000/10000 [==============================] - 3s 330us/sample - loss: 1.4404 - acc: 0.6836\n",
      "\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 0.00024299999931827186.\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 1.6990 - acc: 0.8260 - val_loss: 1.4322 - val_acc: 0.6836\n",
      "Epoch 109/120\n",
      "10000/10000 [==============================] - 3s 331us/sample - loss: 1.4393 - acc: 0.6835\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 1.6932 - acc: 0.8277 - val_loss: 1.4312 - val_acc: 0.6835\n",
      "Epoch 110/120\n",
      "10000/10000 [==============================] - 3s 324us/sample - loss: 1.4398 - acc: 0.6837\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 1.7084 - acc: 0.8251 - val_loss: 1.4318 - val_acc: 0.6837\n",
      "Epoch 111/120\n",
      "10000/10000 [==============================] - 3s 324us/sample - loss: 1.4392 - acc: 0.6840\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.7002 - acc: 0.8254 - val_loss: 1.4311 - val_acc: 0.6840\n",
      "Epoch 112/120\n",
      "10000/10000 [==============================] - 3s 326us/sample - loss: 1.4367 - acc: 0.6842\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.6918 - acc: 0.8245 - val_loss: 1.4286 - val_acc: 0.6842\n",
      "Epoch 113/120\n",
      "10000/10000 [==============================] - 3s 326us/sample - loss: 1.4369 - acc: 0.6841\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.7003 - acc: 0.8245 - val_loss: 1.4288 - val_acc: 0.6841\n",
      "Epoch 114/120\n",
      "10000/10000 [==============================] - 3s 328us/sample - loss: 1.4378 - acc: 0.6828\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.7022 - acc: 0.8259 - val_loss: 1.4298 - val_acc: 0.6828\n",
      "Epoch 115/120\n",
      "10000/10000 [==============================] - 3s 325us/sample - loss: 1.4378 - acc: 0.6831\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.6924 - acc: 0.8257 - val_loss: 1.4300 - val_acc: 0.6831\n",
      "Epoch 116/120\n",
      "10000/10000 [==============================] - 3s 324us/sample - loss: 1.4370 - acc: 0.6824\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 1.7119 - acc: 0.8223 - val_loss: 1.4291 - val_acc: 0.6824\n",
      "Epoch 117/120\n",
      "10000/10000 [==============================] - 3s 329us/sample - loss: 1.4366 - acc: 0.6829\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.6997 - acc: 0.8271 - val_loss: 1.4286 - val_acc: 0.6829\n",
      "Epoch 118/120\n",
      "10000/10000 [==============================] - 3s 328us/sample - loss: 1.4376 - acc: 0.6806\n",
      "\n",
      "Epoch 00118: ReduceLROnPlateau reducing learning rate to 7.290000066859647e-05.\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.7026 - acc: 0.8251 - val_loss: 1.4296 - val_acc: 0.6806\n",
      "Epoch 119/120\n",
      "10000/10000 [==============================] - 3s 329us/sample - loss: 1.4370 - acc: 0.6816\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.6999 - acc: 0.8250 - val_loss: 1.4289 - val_acc: 0.6816\n",
      "Epoch 120/120\n",
      "10000/10000 [==============================] - 3s 330us/sample - loss: 1.4372 - acc: 0.6814\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 1.6983 - acc: 0.8262 - val_loss: 1.4293 - val_acc: 0.6814\n"
     ]
    }
   ],
   "source": [
    "model = resnet.get_model()\n",
    "optimizer = SGD(lr=learning_rate, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "log= TensorBoard(log_dir=\"./logs/mixup\")\n",
    "\n",
    "callbacks = [log,EarlyStopping(monitor = 'val_loss', patience = 13), \n",
    "             ReduceLROnPlateau(monitor = 'val_loss', factor = 0.3, patience = 6, verbose = 1,min_lr=1e-5)]\n",
    "\n",
    "datagen = ImageDataGenerator(width_shift_range=0.15,\n",
    "                             height_shift_range=0.15,\n",
    "                             horizontal_flip=True)\n",
    "\n",
    "training_generator = MixupGenerator(x_train, y_train, batch_size=batch_size, alpha=0.4, datagen=datagen)()\n",
    "model.fit(training_generator,\n",
    "          steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "          validation_data=(x_val, y_val),\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model.save('model_mixup.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b74a31",
   "metadata": {},
   "source": [
    "### Cutmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42059e65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "10000/10000 [==============================] - 4s 432us/sample - loss: 4.2890 - acc: 0.1091\n",
      "312/312 [==============================] - 55s 177ms/step - loss: 4.7882 - acc: 0.0540 - val_loss: 4.2896 - val_acc: 0.1091\n",
      "Epoch 2/120\n",
      "10000/10000 [==============================] - 3s 344us/sample - loss: 3.9251 - acc: 0.1483\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 4.4708 - acc: 0.0964 - val_loss: 3.9187 - val_acc: 0.1483\n",
      "Epoch 3/120\n",
      "10000/10000 [==============================] - 3s 341us/sample - loss: 3.6205 - acc: 0.2066\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 4.3109 - acc: 0.1269 - val_loss: 3.6176 - val_acc: 0.2066\n",
      "Epoch 4/120\n",
      "10000/10000 [==============================] - 3s 342us/sample - loss: 3.4914 - acc: 0.2358\n",
      "312/312 [==============================] - 52s 168ms/step - loss: 4.2179 - acc: 0.1400 - val_loss: 3.4858 - val_acc: 0.2358\n",
      "Epoch 5/120\n",
      "10000/10000 [==============================] - 3s 341us/sample - loss: 3.3353 - acc: 0.2521\n",
      "312/312 [==============================] - 54s 172ms/step - loss: 4.0725 - acc: 0.1674 - val_loss: 3.3327 - val_acc: 0.2521\n",
      "Epoch 6/120\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 3.1501 - acc: 0.2913\n",
      "312/312 [==============================] - 52s 168ms/step - loss: 4.0165 - acc: 0.1780 - val_loss: 3.1457 - val_acc: 0.2913\n",
      "Epoch 7/120\n",
      "10000/10000 [==============================] - 3s 341us/sample - loss: 3.0277 - acc: 0.3084\n",
      "312/312 [==============================] - 52s 168ms/step - loss: 3.9188 - acc: 0.1954 - val_loss: 3.0253 - val_acc: 0.3084\n",
      "Epoch 8/120\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 2.7862 - acc: 0.3603\n",
      "312/312 [==============================] - 52s 168ms/step - loss: 3.8227 - acc: 0.2235 - val_loss: 2.7799 - val_acc: 0.3603\n",
      "Epoch 9/120\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 2.7699 - acc: 0.3561\n",
      "312/312 [==============================] - 52s 168ms/step - loss: 3.8161 - acc: 0.2184 - val_loss: 2.7684 - val_acc: 0.3561\n",
      "Epoch 10/120\n",
      "10000/10000 [==============================] - 4s 351us/sample - loss: 2.7801 - acc: 0.3579\n",
      "312/312 [==============================] - 53s 171ms/step - loss: 3.8124 - acc: 0.2181 - val_loss: 2.7772 - val_acc: 0.3579\n",
      "Epoch 11/120\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 2.6792 - acc: 0.3754\n",
      "312/312 [==============================] - 54s 174ms/step - loss: 3.6855 - acc: 0.2464 - val_loss: 2.6801 - val_acc: 0.3754\n",
      "Epoch 12/120\n",
      "10000/10000 [==============================] - 3s 343us/sample - loss: 2.5791 - acc: 0.3983\n",
      "312/312 [==============================] - 53s 171ms/step - loss: 3.6702 - acc: 0.2499 - val_loss: 2.5767 - val_acc: 0.3983\n",
      "Epoch 13/120\n",
      "10000/10000 [==============================] - 3s 342us/sample - loss: 2.5257 - acc: 0.4217\n",
      "312/312 [==============================] - 53s 171ms/step - loss: 3.6425 - acc: 0.2587 - val_loss: 2.5230 - val_acc: 0.4217\n",
      "Epoch 14/120\n",
      "10000/10000 [==============================] - 3s 343us/sample - loss: 2.4758 - acc: 0.4204\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 3.5667 - acc: 0.2794 - val_loss: 2.4742 - val_acc: 0.4204\n",
      "Epoch 15/120\n",
      "10000/10000 [==============================] - 3s 342us/sample - loss: 2.5337 - acc: 0.4281\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 3.5522 - acc: 0.2839 - val_loss: 2.5317 - val_acc: 0.4281\n",
      "Epoch 16/120\n",
      "10000/10000 [==============================] - 3s 342us/sample - loss: 2.4642 - acc: 0.4316\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 3.5191 - acc: 0.2903 - val_loss: 2.4592 - val_acc: 0.4316\n",
      "Epoch 17/120\n",
      "10000/10000 [==============================] - 3s 343us/sample - loss: 2.3391 - acc: 0.4607\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 3.4951 - acc: 0.2889 - val_loss: 2.3350 - val_acc: 0.4607\n",
      "Epoch 18/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 2.2709 - acc: 0.4726\n",
      "312/312 [==============================] - 52s 168ms/step - loss: 3.4544 - acc: 0.3085 - val_loss: 2.2680 - val_acc: 0.4726\n",
      "Epoch 19/120\n",
      "10000/10000 [==============================] - 3s 333us/sample - loss: 2.4189 - acc: 0.4358\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 3.4494 - acc: 0.3059 - val_loss: 2.4165 - val_acc: 0.4358\n",
      "Epoch 20/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 2.2742 - acc: 0.4691\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 3.4056 - acc: 0.3162 - val_loss: 2.2732 - val_acc: 0.4691\n",
      "Epoch 21/120\n",
      "10000/10000 [==============================] - 3s 334us/sample - loss: 2.3089 - acc: 0.4640\n",
      "312/312 [==============================] - 52s 165ms/step - loss: 3.4217 - acc: 0.3164 - val_loss: 2.3060 - val_acc: 0.4640\n",
      "Epoch 22/120\n",
      "10000/10000 [==============================] - 3s 333us/sample - loss: 2.2363 - acc: 0.4977\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 3.3720 - acc: 0.3276 - val_loss: 2.2341 - val_acc: 0.4977\n",
      "Epoch 23/120\n",
      "10000/10000 [==============================] - 3s 342us/sample - loss: 2.4117 - acc: 0.4560\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 3.4123 - acc: 0.3220 - val_loss: 2.4092 - val_acc: 0.4560\n",
      "Epoch 24/120\n",
      "10000/10000 [==============================] - 3s 334us/sample - loss: 2.4397 - acc: 0.4522\n",
      "312/312 [==============================] - 52s 168ms/step - loss: 3.3621 - acc: 0.3425 - val_loss: 2.4351 - val_acc: 0.4522\n",
      "Epoch 25/120\n",
      "10000/10000 [==============================] - 3s 328us/sample - loss: 2.6186 - acc: 0.4236\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 3.3045 - acc: 0.3501 - val_loss: 2.6146 - val_acc: 0.4236\n",
      "Epoch 26/120\n",
      "10000/10000 [==============================] - 3s 343us/sample - loss: 2.8206 - acc: 0.3944\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 3.3215 - acc: 0.3485 - val_loss: 2.8206 - val_acc: 0.3944\n",
      "Epoch 27/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 2.2160 - acc: 0.4862\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 3.2760 - acc: 0.3537 - val_loss: 2.2220 - val_acc: 0.4862\n",
      "Epoch 28/120\n",
      "10000/10000 [==============================] - 3s 329us/sample - loss: 2.2690 - acc: 0.4804\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 3.2869 - acc: 0.3591 - val_loss: 2.2703 - val_acc: 0.4804\n",
      "Epoch 29/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 2.2974 - acc: 0.4915\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 3.3400 - acc: 0.3547 - val_loss: 2.2949 - val_acc: 0.4915\n",
      "Epoch 30/120\n",
      "10000/10000 [==============================] - 3s 331us/sample - loss: 2.4029 - acc: 0.4492\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 3.3449 - acc: 0.3385 - val_loss: 2.4089 - val_acc: 0.4492\n",
      "Epoch 31/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 2.2997 - acc: 0.4928\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 3.3060 - acc: 0.3440 - val_loss: 2.2954 - val_acc: 0.4928\n",
      "Epoch 32/120\n",
      "10000/10000 [==============================] - 3s 331us/sample - loss: 2.1853 - acc: 0.4989\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 3.2689 - acc: 0.3593 - val_loss: 2.1857 - val_acc: 0.4989\n",
      "Epoch 33/120\n",
      "10000/10000 [==============================] - 3s 337us/sample - loss: 2.1943 - acc: 0.5065\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 3.2294 - acc: 0.3769 - val_loss: 2.1913 - val_acc: 0.5065\n",
      "Epoch 34/120\n",
      "10000/10000 [==============================] - 3s 330us/sample - loss: 2.2931 - acc: 0.5059\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 3.2377 - acc: 0.3719 - val_loss: 2.2894 - val_acc: 0.5059\n",
      "Epoch 35/120\n",
      "10000/10000 [==============================] - 3s 330us/sample - loss: 2.3221 - acc: 0.4820\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 3.2297 - acc: 0.3694 - val_loss: 2.3200 - val_acc: 0.4820\n",
      "Epoch 36/120\n",
      "10000/10000 [==============================] - 3s 337us/sample - loss: 2.0683 - acc: 0.5279\n",
      "312/312 [==============================] - 52s 165ms/step - loss: 3.2809 - acc: 0.3683 - val_loss: 2.0690 - val_acc: 0.5279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/120\n",
      "10000/10000 [==============================] - 3s 338us/sample - loss: 2.1641 - acc: 0.5075\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 3.1342 - acc: 0.4022 - val_loss: 2.1634 - val_acc: 0.5075\n",
      "Epoch 38/120\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 2.0874 - acc: 0.5247\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 3.2109 - acc: 0.3923 - val_loss: 2.0822 - val_acc: 0.5247\n",
      "Epoch 39/120\n",
      "10000/10000 [==============================] - 3s 331us/sample - loss: 2.1077 - acc: 0.5259\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 3.2556 - acc: 0.3758 - val_loss: 2.1055 - val_acc: 0.5259\n",
      "Epoch 40/120\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 2.0941 - acc: 0.5197\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 3.2339 - acc: 0.3757 - val_loss: 2.0897 - val_acc: 0.5197\n",
      "Epoch 41/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 2.0394 - acc: 0.5411\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 3.2144 - acc: 0.3690 - val_loss: 2.0381 - val_acc: 0.5411\n",
      "Epoch 42/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 2.2554 - acc: 0.5147\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 3.2640 - acc: 0.3687 - val_loss: 2.2521 - val_acc: 0.5147\n",
      "Epoch 43/120\n",
      "10000/10000 [==============================] - 3s 328us/sample - loss: 2.0983 - acc: 0.5380\n",
      "312/312 [==============================] - 51s 162ms/step - loss: 3.1818 - acc: 0.3959 - val_loss: 2.0942 - val_acc: 0.5380\n",
      "Epoch 44/120\n",
      "10000/10000 [==============================] - 3s 330us/sample - loss: 2.0986 - acc: 0.5427\n",
      "312/312 [==============================] - 51s 162ms/step - loss: 3.1472 - acc: 0.4062 - val_loss: 2.0954 - val_acc: 0.5427\n",
      "Epoch 45/120\n",
      "10000/10000 [==============================] - 3s 329us/sample - loss: 2.3116 - acc: 0.4895\n",
      "312/312 [==============================] - 51s 162ms/step - loss: 3.1653 - acc: 0.4057 - val_loss: 2.3072 - val_acc: 0.4895\n",
      "Epoch 46/120\n",
      "10000/10000 [==============================] - 3s 327us/sample - loss: 2.1946 - acc: 0.5188\n",
      "312/312 [==============================] - 51s 162ms/step - loss: 3.2227 - acc: 0.3911 - val_loss: 2.1928 - val_acc: 0.5188\n",
      "Epoch 47/120\n",
      "10000/10000 [==============================] - 3s 331us/sample - loss: 2.0300 - acc: 0.5486\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 3.1773 - acc: 0.3927 - val_loss: 2.0253 - val_acc: 0.5486\n",
      "Epoch 48/120\n",
      "10000/10000 [==============================] - 3s 336us/sample - loss: 2.0544 - acc: 0.5501\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 3.1751 - acc: 0.3938 - val_loss: 2.0532 - val_acc: 0.5501\n",
      "Epoch 49/120\n",
      "10000/10000 [==============================] - 3s 333us/sample - loss: 2.2418 - acc: 0.5135\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 3.1875 - acc: 0.3988 - val_loss: 2.2423 - val_acc: 0.5135\n",
      "Epoch 50/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 2.0368 - acc: 0.5381\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 3.1718 - acc: 0.4008 - val_loss: 2.0360 - val_acc: 0.5381\n",
      "Epoch 51/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 2.1760 - acc: 0.5147\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 3.1736 - acc: 0.4128 - val_loss: 2.1804 - val_acc: 0.5147\n",
      "Epoch 52/120\n",
      "10000/10000 [==============================] - 3s 331us/sample - loss: 2.1791 - acc: 0.5274\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 3.1376 - acc: 0.4118 - val_loss: 2.1788 - val_acc: 0.5274\n",
      "Epoch 53/120\n",
      "10000/10000 [==============================] - 3s 331us/sample - loss: 2.1396 - acc: 0.5321\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
      "312/312 [==============================] - 58s 184ms/step - loss: 3.2600 - acc: 0.3750 - val_loss: 2.1404 - val_acc: 0.5321\n",
      "Epoch 54/120\n",
      "10000/10000 [==============================] - 3s 330us/sample - loss: 1.8635 - acc: 0.6069\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 3.0046 - acc: 0.4451 - val_loss: 1.8635 - val_acc: 0.6069\n",
      "Epoch 55/120\n",
      "10000/10000 [==============================] - 3s 328us/sample - loss: 1.7734 - acc: 0.6215\n",
      "312/312 [==============================] - 51s 162ms/step - loss: 2.9306 - acc: 0.4626 - val_loss: 1.7729 - val_acc: 0.6215\n",
      "Epoch 56/120\n",
      "10000/10000 [==============================] - 3s 342us/sample - loss: 1.7964 - acc: 0.6220\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.8643 - acc: 0.4796 - val_loss: 1.7972 - val_acc: 0.6220\n",
      "Epoch 57/120\n",
      "10000/10000 [==============================] - 3s 334us/sample - loss: 1.7509 - acc: 0.6297\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.8912 - acc: 0.4568 - val_loss: 1.7503 - val_acc: 0.6297\n",
      "Epoch 58/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 1.8635 - acc: 0.6085\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.8559 - acc: 0.4516 - val_loss: 1.8638 - val_acc: 0.6085\n",
      "Epoch 59/120\n",
      "10000/10000 [==============================] - 3s 333us/sample - loss: 1.7493 - acc: 0.6246\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.7791 - acc: 0.4923 - val_loss: 1.7472 - val_acc: 0.6246\n",
      "Epoch 60/120\n",
      "10000/10000 [==============================] - 3s 329us/sample - loss: 1.6526 - acc: 0.6427\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.8343 - acc: 0.4831 - val_loss: 1.6529 - val_acc: 0.6427\n",
      "Epoch 61/120\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 1.7072 - acc: 0.6284\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 2.8236 - acc: 0.4800 - val_loss: 1.7089 - val_acc: 0.6284\n",
      "Epoch 62/120\n",
      "10000/10000 [==============================] - 3s 336us/sample - loss: 1.7651 - acc: 0.6161\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.7733 - acc: 0.4837 - val_loss: 1.7640 - val_acc: 0.6161\n",
      "Epoch 63/120\n",
      "10000/10000 [==============================] - 3s 343us/sample - loss: 1.6428 - acc: 0.6396\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.7775 - acc: 0.4834 - val_loss: 1.6425 - val_acc: 0.6396\n",
      "Epoch 64/120\n",
      "10000/10000 [==============================] - 3s 331us/sample - loss: 1.6841 - acc: 0.6361\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.7631 - acc: 0.4959 - val_loss: 1.6808 - val_acc: 0.6361\n",
      "Epoch 65/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 1.7402 - acc: 0.6262\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.7720 - acc: 0.4800 - val_loss: 1.7391 - val_acc: 0.6262\n",
      "Epoch 66/120\n",
      "10000/10000 [==============================] - 3s 329us/sample - loss: 1.7045 - acc: 0.6291\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.7848 - acc: 0.4723 - val_loss: 1.7028 - val_acc: 0.6291\n",
      "Epoch 67/120\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 1.6643 - acc: 0.6340\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.7763 - acc: 0.4773 - val_loss: 1.6630 - val_acc: 0.6340\n",
      "Epoch 68/120\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 1.7325 - acc: 0.6258\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.8024 - acc: 0.4723 - val_loss: 1.7311 - val_acc: 0.6258\n",
      "Epoch 69/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 1.6380 - acc: 0.6352\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.7207 - acc: 0.4952 - val_loss: 1.6381 - val_acc: 0.6352\n",
      "Epoch 70/120\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 1.6190 - acc: 0.6494\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.7021 - acc: 0.4965 - val_loss: 1.6166 - val_acc: 0.6494\n",
      "Epoch 71/120\n",
      "10000/10000 [==============================] - 3s 338us/sample - loss: 1.7683 - acc: 0.6218\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.7471 - acc: 0.4899 - val_loss: 1.7673 - val_acc: 0.6218\n",
      "Epoch 72/120\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 1.6622 - acc: 0.6317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 52s 165ms/step - loss: 2.6869 - acc: 0.4941 - val_loss: 1.6621 - val_acc: 0.6317\n",
      "Epoch 73/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 1.6276 - acc: 0.6350\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.6730 - acc: 0.4903 - val_loss: 1.6277 - val_acc: 0.6350\n",
      "Epoch 74/120\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 1.6662 - acc: 0.6333\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.7091 - acc: 0.5021 - val_loss: 1.6658 - val_acc: 0.6333\n",
      "Epoch 75/120\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 1.6761 - acc: 0.6356\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.7181 - acc: 0.4794 - val_loss: 1.6724 - val_acc: 0.6356\n",
      "Epoch 76/120\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 1.5821 - acc: 0.6450\n",
      "312/312 [==============================] - 52s 165ms/step - loss: 2.6685 - acc: 0.5063 - val_loss: 1.5835 - val_acc: 0.6450\n",
      "Epoch 77/120\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 1.6288 - acc: 0.6345\n",
      "312/312 [==============================] - 52s 165ms/step - loss: 2.6533 - acc: 0.5072 - val_loss: 1.6284 - val_acc: 0.6345\n",
      "Epoch 78/120\n",
      "10000/10000 [==============================] - 3s 327us/sample - loss: 1.6384 - acc: 0.6262\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.6729 - acc: 0.4848 - val_loss: 1.6390 - val_acc: 0.6262\n",
      "Epoch 79/120\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 1.6876 - acc: 0.6201\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.6585 - acc: 0.5057 - val_loss: 1.6841 - val_acc: 0.6201\n",
      "Epoch 80/120\n",
      "10000/10000 [==============================] - 3s 331us/sample - loss: 1.5919 - acc: 0.6398\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.6961 - acc: 0.4857 - val_loss: 1.5930 - val_acc: 0.6398\n",
      "Epoch 81/120\n",
      "10000/10000 [==============================] - 3s 341us/sample - loss: 1.6751 - acc: 0.6377\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.6229 - acc: 0.5060 - val_loss: 1.6738 - val_acc: 0.6377\n",
      "Epoch 82/120\n",
      "10000/10000 [==============================] - 3s 338us/sample - loss: 1.5899 - acc: 0.6406\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.6849 - acc: 0.4820 - val_loss: 1.5914 - val_acc: 0.6406\n",
      "Epoch 83/120\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 1.5280 - acc: 0.6681\n",
      "312/312 [==============================] - 52s 165ms/step - loss: 2.6106 - acc: 0.5228 - val_loss: 1.5295 - val_acc: 0.6681\n",
      "Epoch 84/120\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 1.5111 - acc: 0.6678\n",
      "312/312 [==============================] - 52s 165ms/step - loss: 2.5737 - acc: 0.5333 - val_loss: 1.5133 - val_acc: 0.6678\n",
      "Epoch 85/120\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 1.5164 - acc: 0.6764\n",
      "312/312 [==============================] - 52s 165ms/step - loss: 2.6100 - acc: 0.5058 - val_loss: 1.5183 - val_acc: 0.6764\n",
      "Epoch 86/120\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 1.4918 - acc: 0.6690\n",
      "312/312 [==============================] - 52s 165ms/step - loss: 2.5950 - acc: 0.5202 - val_loss: 1.4924 - val_acc: 0.6690\n",
      "Epoch 87/120\n",
      "10000/10000 [==============================] - 3s 331us/sample - loss: 1.4803 - acc: 0.6743\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.5103 - acc: 0.5350 - val_loss: 1.4827 - val_acc: 0.6743\n",
      "Epoch 88/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 1.4889 - acc: 0.6700\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.5401 - acc: 0.5111 - val_loss: 1.4901 - val_acc: 0.6700\n",
      "Epoch 89/120\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 1.4730 - acc: 0.6741\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.4733 - acc: 0.5529 - val_loss: 1.4746 - val_acc: 0.6741\n",
      "Epoch 90/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 1.4877 - acc: 0.6780\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.5593 - acc: 0.5353 - val_loss: 1.4880 - val_acc: 0.6780\n",
      "Epoch 91/120\n",
      "10000/10000 [==============================] - 3s 333us/sample - loss: 1.4938 - acc: 0.6728\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.4830 - acc: 0.5458 - val_loss: 1.4945 - val_acc: 0.6728\n",
      "Epoch 92/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 1.4478 - acc: 0.6787\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.4932 - acc: 0.5447 - val_loss: 1.4489 - val_acc: 0.6787\n",
      "Epoch 93/120\n",
      "10000/10000 [==============================] - 3s 336us/sample - loss: 1.4746 - acc: 0.6782\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.4463 - acc: 0.5467 - val_loss: 1.4756 - val_acc: 0.6782\n",
      "Epoch 94/120\n",
      "10000/10000 [==============================] - 4s 351us/sample - loss: 1.4765 - acc: 0.6748\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.4805 - acc: 0.5456 - val_loss: 1.4772 - val_acc: 0.6748\n",
      "Epoch 95/120\n",
      "10000/10000 [==============================] - 3s 331us/sample - loss: 1.4580 - acc: 0.6788\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.4789 - acc: 0.5402 - val_loss: 1.4593 - val_acc: 0.6788\n",
      "Epoch 96/120\n",
      "10000/10000 [==============================] - 3s 330us/sample - loss: 1.4784 - acc: 0.6759\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.5281 - acc: 0.5370 - val_loss: 1.4778 - val_acc: 0.6759\n",
      "Epoch 97/120\n",
      "10000/10000 [==============================] - 3s 346us/sample - loss: 1.4796 - acc: 0.6769\n",
      "312/312 [==============================] - 53s 171ms/step - loss: 2.5431 - acc: 0.5243 - val_loss: 1.4804 - val_acc: 0.6769\n",
      "Epoch 98/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 1.4445 - acc: 0.6783\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.4754 - acc: 0.5500 - val_loss: 1.4456 - val_acc: 0.6783\n",
      "Epoch 99/120\n",
      "10000/10000 [==============================] - 3s 329us/sample - loss: 1.4399 - acc: 0.6797\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.4366 - acc: 0.5686 - val_loss: 1.4412 - val_acc: 0.6797\n",
      "Epoch 100/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 1.4617 - acc: 0.6772\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.4947 - acc: 0.5429 - val_loss: 1.4625 - val_acc: 0.6772\n",
      "Epoch 101/120\n",
      "10000/10000 [==============================] - 3s 329us/sample - loss: 1.4410 - acc: 0.6813\n",
      "312/312 [==============================] - 52s 165ms/step - loss: 2.3768 - acc: 0.5740 - val_loss: 1.4412 - val_acc: 0.6813\n",
      "Epoch 102/120\n",
      "10000/10000 [==============================] - 3s 330us/sample - loss: 1.4300 - acc: 0.6821\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.4632 - acc: 0.5454 - val_loss: 1.4311 - val_acc: 0.6821\n",
      "Epoch 103/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 1.4697 - acc: 0.6845\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.5352 - acc: 0.5173 - val_loss: 1.4713 - val_acc: 0.6845\n",
      "Epoch 104/120\n",
      "10000/10000 [==============================] - 3s 333us/sample - loss: 1.4436 - acc: 0.6797\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.4566 - acc: 0.5498 - val_loss: 1.4444 - val_acc: 0.6797\n",
      "Epoch 105/120\n",
      "10000/10000 [==============================] - 3s 331us/sample - loss: 1.4505 - acc: 0.6777\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.5293 - acc: 0.5406 - val_loss: 1.4510 - val_acc: 0.6777\n",
      "Epoch 106/120\n",
      "10000/10000 [==============================] - 3s 329us/sample - loss: 1.4638 - acc: 0.6796\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.4767 - acc: 0.5530 - val_loss: 1.4626 - val_acc: 0.6796\n",
      "Epoch 107/120\n",
      "10000/10000 [==============================] - 3s 329us/sample - loss: 1.4689 - acc: 0.6813\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.5280 - acc: 0.5105 - val_loss: 1.4692 - val_acc: 0.6813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/120\n",
      "10000/10000 [==============================] - 3s 333us/sample - loss: 1.4645 - acc: 0.6782\n",
      "\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.4612 - acc: 0.5418 - val_loss: 1.4657 - val_acc: 0.6782\n",
      "Epoch 109/120\n",
      "10000/10000 [==============================] - 3s 331us/sample - loss: 1.4216 - acc: 0.6854\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.4715 - acc: 0.5398 - val_loss: 1.4240 - val_acc: 0.6854\n",
      "Epoch 110/120\n",
      "10000/10000 [==============================] - 3s 330us/sample - loss: 1.4183 - acc: 0.6846\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.4510 - acc: 0.5377 - val_loss: 1.4200 - val_acc: 0.6846\n",
      "Epoch 111/120\n",
      "10000/10000 [==============================] - 3s 330us/sample - loss: 1.4292 - acc: 0.6850\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.4117 - acc: 0.5660 - val_loss: 1.4302 - val_acc: 0.6850\n",
      "Epoch 112/120\n",
      "10000/10000 [==============================] - 3s 330us/sample - loss: 1.4275 - acc: 0.6859\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.4854 - acc: 0.5416 - val_loss: 1.4285 - val_acc: 0.6859\n",
      "Epoch 113/120\n",
      "10000/10000 [==============================] - 3s 330us/sample - loss: 1.4067 - acc: 0.6873\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.4430 - acc: 0.5557 - val_loss: 1.4079 - val_acc: 0.6873\n",
      "Epoch 114/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 1.4137 - acc: 0.6876\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.4085 - acc: 0.5604 - val_loss: 1.4149 - val_acc: 0.6876\n",
      "Epoch 115/120\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 1.4128 - acc: 0.6861\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.4118 - acc: 0.5645 - val_loss: 1.4137 - val_acc: 0.6861\n",
      "Epoch 116/120\n",
      "10000/10000 [==============================] - 3s 331us/sample - loss: 1.4084 - acc: 0.6866\n",
      "312/312 [==============================] - 51s 162ms/step - loss: 2.3428 - acc: 0.5727 - val_loss: 1.4100 - val_acc: 0.6866\n",
      "Epoch 117/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 1.3953 - acc: 0.6900\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.4082 - acc: 0.5735 - val_loss: 1.3965 - val_acc: 0.6900\n",
      "Epoch 118/120\n",
      "10000/10000 [==============================] - 3s 331us/sample - loss: 1.3950 - acc: 0.6884\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.3615 - acc: 0.5635 - val_loss: 1.3965 - val_acc: 0.6884\n",
      "Epoch 119/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 1.4229 - acc: 0.6860\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.4391 - acc: 0.5474 - val_loss: 1.4237 - val_acc: 0.6860\n",
      "Epoch 120/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 1.4317 - acc: 0.6857\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 2.4696 - acc: 0.5451 - val_loss: 1.4328 - val_acc: 0.6857\n"
     ]
    }
   ],
   "source": [
    "model = resnet.get_model()\n",
    "optimizer = SGD(lr=learning_rate, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "log= TensorBoard(log_dir=\"./logs/cutmix\")\n",
    "\n",
    "callbacks = [log,EarlyStopping(monitor = 'val_loss', patience = 13), \n",
    "             ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 6, verbose = 1,min_lr=1e-5)]\n",
    "\n",
    "datagen = ImageDataGenerator(width_shift_range=0.15,\n",
    "                             height_shift_range=0.15,\n",
    "                             horizontal_flip=True)\n",
    "\n",
    "training_generator = CutMixGenerator(x_train, y_train, batch_size=batch_size, alpha=0.8, datagen=datagen)()\n",
    "model.fit(training_generator,\n",
    "          steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "          validation_data=(x_val, y_val),\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model.save('model_cutmix.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c26ba",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dc44b89",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def evaluate(model,x_test,y_test):\n",
    "    loss,acc=model.evaluate(x_test,y_test)\n",
    "    return loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e087747d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 6s 638us/sample - loss: 2.6015 - acc: 0.5659\n",
      "baseline without aug :test loss 2.6015  test acc:0.5659\n",
      "\n",
      "10000/10000 [==============================] - 6s 647us/sample - loss: 1.9347 - acc: 0.6669\n",
      "baseline: test loss 1.9347  test acc:0.6669\n",
      "\n",
      "10000/10000 [==============================] - 7s 682us/sample - loss: 1.7301 - acc: 0.6745\n",
      "baseline  with cutout: test loss 1.7301  test acc:0.6745\n",
      "\n",
      "10000/10000 [==============================] - 7s 725us/sample - loss: 1.4141 - acc: 0.6896\n",
      "baseline  with mixup: test loss 1.4141  test acc:0.6896\n",
      "\n",
      "10000/10000 [==============================] - 8s 756us/sample - loss: 1.4149 - acc: 0.6923\n",
      "baseline with cutmix: test loss 1.4149  test acc:0.6923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prnt = ('test loss {:0.4f}  test acc:{:0.4f}')\n",
    "\n",
    "model_base=load_model(\"model_withoutaug.h5\")\n",
    "loss,acc=evaluate(model_base,x_test,y_test)\n",
    "print(\"baseline without aug :\"+prnt.format(loss,acc)+'\\n')\n",
    "\n",
    "model_baseline=load_model(\"model_baseline.h5\")\n",
    "loss,acc=evaluate(model_baseline,x_test,y_test)\n",
    "print(\"baseline: \"+prnt.format(loss,acc)+'\\n')\n",
    "\n",
    "model_cutout=load_model(\"model_cutout.h5\")\n",
    "loss,acc=evaluate(model_cutout,x_test,y_test)\n",
    "print(\"baseline  with cutout: \"+prnt.format(loss,acc)+'\\n')\n",
    "\n",
    "model_mixup=load_model(\"model_mixup.h5\")\n",
    "loss,acc=evaluate(model_mixup,x_test,y_test)\n",
    "print(\"baseline  with mixup: \"+prnt.format(loss,acc)+'\\n')\n",
    "\n",
    "model_cutmix=load_model(\"model_cutmix.h5\")\n",
    "loss,acc=evaluate(model_cutmix,x_test,y_test)\n",
    "print(\"baseline with cutmix: \"+prnt.format(loss,acc)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d125750",
   "metadata": {},
   "source": [
    "# Further more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37737d18",
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cutout(cutout_mask_size = 8):\n",
    "    def cut(input_img):\n",
    "        image = np.copy(input_img)\n",
    "        mask_value = image.mean()\n",
    "        \n",
    "        \n",
    "        h, w, _ = image.shape\n",
    "        \n",
    "        y = np.random.randint(h)\n",
    "        x = np.random.randint(w)\n",
    "        \n",
    "        \n",
    "        top = np.clip(y - cutout_mask_size // 2, 0, h)\n",
    "        bottom  = np.clip(y + cutout_mask_size // 2, 0, h)\n",
    "        left = np.clip(x - cutout_mask_size // 2, 0, w)\n",
    "        right = np.clip(x + cutout_mask_size // 2, 0, w)\n",
    "        \n",
    "        image[top:bottom, left:right, :].fill(mask_value)\n",
    "        return image\n",
    "\n",
    "    return cut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a48254d",
   "metadata": {},
   "source": [
    "## cutout + mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed9fb38e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "10000/10000 [==============================] - 5s 465us/sample - loss: 4.7789 - acc: 0.0945\n",
      "312/312 [==============================] - 54s 174ms/step - loss: 4.5575 - acc: 0.0846 - val_loss: 4.7787 - val_acc: 0.0945\n",
      "Epoch 2/120\n",
      "10000/10000 [==============================] - 3s 330us/sample - loss: 3.9404 - acc: 0.1673\n",
      "312/312 [==============================] - 49s 156ms/step - loss: 4.1408 - acc: 0.1554 - val_loss: 3.9396 - val_acc: 0.1673\n",
      "Epoch 3/120\n",
      "10000/10000 [==============================] - 3s 336us/sample - loss: 3.5394 - acc: 0.2293\n",
      "312/312 [==============================] - 50s 159ms/step - loss: 3.9152 - acc: 0.2007 - val_loss: 3.5438 - val_acc: 0.2293\n",
      "Epoch 4/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 3.2225 - acc: 0.2822\n",
      "312/312 [==============================] - 51s 162ms/step - loss: 3.7226 - acc: 0.2415 - val_loss: 3.2208 - val_acc: 0.2822\n",
      "Epoch 5/120\n",
      "10000/10000 [==============================] - 3s 343us/sample - loss: 2.9562 - acc: 0.3244\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 3.5948 - acc: 0.2665 - val_loss: 2.9533 - val_acc: 0.3244\n",
      "Epoch 6/120\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 2.8856 - acc: 0.3425\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 3.4713 - acc: 0.2979 - val_loss: 2.8815 - val_acc: 0.3425\n",
      "Epoch 7/120\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 2.7442 - acc: 0.3668\n",
      "312/312 [==============================] - 52s 165ms/step - loss: 3.3780 - acc: 0.3179 - val_loss: 2.7395 - val_acc: 0.3668\n",
      "Epoch 8/120\n",
      "10000/10000 [==============================] - 3s 348us/sample - loss: 2.6175 - acc: 0.3953\n",
      "312/312 [==============================] - 53s 168ms/step - loss: 3.2927 - acc: 0.3398 - val_loss: 2.6132 - val_acc: 0.3953\n",
      "Epoch 9/120\n",
      "10000/10000 [==============================] - 3s 347us/sample - loss: 2.5646 - acc: 0.4099\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 3.2013 - acc: 0.3607 - val_loss: 2.5567 - val_acc: 0.4099\n",
      "Epoch 10/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 2.6335 - acc: 0.3983\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 3.1496 - acc: 0.3771 - val_loss: 2.6266 - val_acc: 0.3983\n",
      "Epoch 11/120\n",
      "10000/10000 [==============================] - 3s 338us/sample - loss: 2.3851 - acc: 0.4506\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 3.0983 - acc: 0.3896 - val_loss: 2.3756 - val_acc: 0.4506\n",
      "Epoch 12/120\n",
      "10000/10000 [==============================] - 3s 346us/sample - loss: 2.4204 - acc: 0.4377\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 3.0347 - acc: 0.4073 - val_loss: 2.4122 - val_acc: 0.4377\n",
      "Epoch 13/120\n",
      "10000/10000 [==============================] - 4s 353us/sample - loss: 2.3613 - acc: 0.4527\n",
      "312/312 [==============================] - 52s 168ms/step - loss: 3.0016 - acc: 0.4182 - val_loss: 2.3508 - val_acc: 0.4527\n",
      "Epoch 14/120\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 2.3368 - acc: 0.4680\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.9684 - acc: 0.4285 - val_loss: 2.3306 - val_acc: 0.4680\n",
      "Epoch 15/120\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 2.1814 - acc: 0.4951\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.9157 - acc: 0.4446 - val_loss: 2.1744 - val_acc: 0.4951\n",
      "Epoch 16/120\n",
      "10000/10000 [==============================] - 3s 339us/sample - loss: 2.3464 - acc: 0.4623\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.8955 - acc: 0.4501 - val_loss: 2.3398 - val_acc: 0.4623\n",
      "Epoch 17/120\n",
      "10000/10000 [==============================] - 3s 338us/sample - loss: 2.2182 - acc: 0.4953\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.8600 - acc: 0.4613 - val_loss: 2.2118 - val_acc: 0.4953\n",
      "Epoch 18/120\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 2.2087 - acc: 0.4973\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 2.8344 - acc: 0.4673 - val_loss: 2.2025 - val_acc: 0.4973\n",
      "Epoch 19/120\n",
      "10000/10000 [==============================] - 3s 336us/sample - loss: 2.1463 - acc: 0.5063\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.8137 - acc: 0.4734 - val_loss: 2.1390 - val_acc: 0.5063\n",
      "Epoch 20/120\n",
      "10000/10000 [==============================] - 3s 337us/sample - loss: 2.1563 - acc: 0.5095\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.7921 - acc: 0.4781 - val_loss: 2.1520 - val_acc: 0.5095\n",
      "Epoch 21/120\n",
      "10000/10000 [==============================] - 3s 346us/sample - loss: 2.1786 - acc: 0.5038\n",
      "312/312 [==============================] - 52s 168ms/step - loss: 2.7714 - acc: 0.4885 - val_loss: 2.1700 - val_acc: 0.5038\n",
      "Epoch 22/120\n",
      "10000/10000 [==============================] - 3s 346us/sample - loss: 2.1826 - acc: 0.5040\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 2.7639 - acc: 0.4920 - val_loss: 2.1761 - val_acc: 0.5040\n",
      "Epoch 23/120\n",
      "10000/10000 [==============================] - 3s 344us/sample - loss: 2.3287 - acc: 0.4793\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.7359 - acc: 0.5000 - val_loss: 2.3201 - val_acc: 0.4793\n",
      "Epoch 24/120\n",
      "10000/10000 [==============================] - 3s 342us/sample - loss: 2.1431 - acc: 0.5136\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 2.7346 - acc: 0.5057 - val_loss: 2.1360 - val_acc: 0.5136\n",
      "Epoch 25/120\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 2.0960 - acc: 0.5309\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 2.7089 - acc: 0.5130 - val_loss: 2.0880 - val_acc: 0.5309\n",
      "Epoch 26/120\n",
      "10000/10000 [==============================] - 3s 341us/sample - loss: 2.1015 - acc: 0.5307\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.6960 - acc: 0.5193 - val_loss: 2.0935 - val_acc: 0.5307\n",
      "Epoch 27/120\n",
      "10000/10000 [==============================] - 3s 341us/sample - loss: 2.0710 - acc: 0.5375\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.6984 - acc: 0.5169 - val_loss: 2.0622 - val_acc: 0.5375\n",
      "Epoch 28/120\n",
      "10000/10000 [==============================] - 3s 343us/sample - loss: 2.0384 - acc: 0.5446\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 2.6893 - acc: 0.5196 - val_loss: 2.0296 - val_acc: 0.5446\n",
      "Epoch 29/120\n",
      "10000/10000 [==============================] - 3s 340us/sample - loss: 2.1010 - acc: 0.5314\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 2.6766 - acc: 0.5281 - val_loss: 2.0910 - val_acc: 0.5314\n",
      "Epoch 30/120\n",
      "10000/10000 [==============================] - 3s 348us/sample - loss: 2.0147 - acc: 0.5592\n",
      "312/312 [==============================] - 52s 168ms/step - loss: 2.6712 - acc: 0.5288 - val_loss: 2.0046 - val_acc: 0.5592\n",
      "Epoch 31/120\n",
      "10000/10000 [==============================] - 4s 354us/sample - loss: 2.0022 - acc: 0.5542\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 2.6527 - acc: 0.5342 - val_loss: 1.9936 - val_acc: 0.5542\n",
      "Epoch 32/120\n",
      "10000/10000 [==============================] - 4s 351us/sample - loss: 2.0289 - acc: 0.5455\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 2.6459 - acc: 0.5376 - val_loss: 2.0188 - val_acc: 0.5455\n",
      "Epoch 33/120\n",
      "10000/10000 [==============================] - 3s 348us/sample - loss: 1.9883 - acc: 0.5556\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 2.6422 - acc: 0.5399 - val_loss: 1.9779 - val_acc: 0.5556\n",
      "Epoch 34/120\n",
      "10000/10000 [==============================] - 3s 350us/sample - loss: 1.9276 - acc: 0.5695\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.6331 - acc: 0.5423 - val_loss: 1.9168 - val_acc: 0.5695\n",
      "Epoch 35/120\n",
      "10000/10000 [==============================] - 3s 348us/sample - loss: 2.0015 - acc: 0.5551\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.6252 - acc: 0.5460 - val_loss: 1.9915 - val_acc: 0.5551\n",
      "Epoch 36/120\n",
      "10000/10000 [==============================] - 3s 348us/sample - loss: 2.0834 - acc: 0.5415\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.6154 - acc: 0.5483 - val_loss: 2.0776 - val_acc: 0.5415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/120\n",
      "10000/10000 [==============================] - 3s 350us/sample - loss: 1.9946 - acc: 0.5673\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.6256 - acc: 0.5483 - val_loss: 1.9857 - val_acc: 0.5673\n",
      "Epoch 38/120\n",
      "10000/10000 [==============================] - 3s 350us/sample - loss: 1.9213 - acc: 0.5783\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.6010 - acc: 0.5537 - val_loss: 1.9124 - val_acc: 0.5783\n",
      "Epoch 39/120\n",
      "10000/10000 [==============================] - 4s 351us/sample - loss: 1.9826 - acc: 0.5682\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.6025 - acc: 0.5546 - val_loss: 1.9755 - val_acc: 0.5682\n",
      "Epoch 40/120\n",
      "10000/10000 [==============================] - 3s 347us/sample - loss: 2.1802 - acc: 0.5304\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.6063 - acc: 0.5554 - val_loss: 2.1714 - val_acc: 0.5304\n",
      "Epoch 41/120\n",
      "10000/10000 [==============================] - 3s 346us/sample - loss: 2.0070 - acc: 0.5646\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.5963 - acc: 0.5587 - val_loss: 1.9978 - val_acc: 0.5646\n",
      "Epoch 42/120\n",
      "10000/10000 [==============================] - 4s 351us/sample - loss: 1.9080 - acc: 0.5822\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.5915 - acc: 0.5622 - val_loss: 1.9003 - val_acc: 0.5822\n",
      "Epoch 43/120\n",
      "10000/10000 [==============================] - 3s 348us/sample - loss: 2.0512 - acc: 0.5545\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.5749 - acc: 0.5666 - val_loss: 2.0424 - val_acc: 0.5545\n",
      "Epoch 44/120\n",
      "10000/10000 [==============================] - 3s 350us/sample - loss: 2.0220 - acc: 0.5608\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.5818 - acc: 0.5643 - val_loss: 2.0125 - val_acc: 0.5608\n",
      "Epoch 45/120\n",
      "10000/10000 [==============================] - 3s 348us/sample - loss: 1.9319 - acc: 0.5774\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.5751 - acc: 0.5720 - val_loss: 1.9215 - val_acc: 0.5774\n",
      "Epoch 46/120\n",
      "10000/10000 [==============================] - 3s 349us/sample - loss: 2.0142 - acc: 0.5630\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.5666 - acc: 0.5751 - val_loss: 2.0095 - val_acc: 0.5630\n",
      "Epoch 47/120\n",
      "10000/10000 [==============================] - 3s 348us/sample - loss: 1.9447 - acc: 0.5802\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.5719 - acc: 0.5729 - val_loss: 1.9360 - val_acc: 0.5802\n",
      "Epoch 48/120\n",
      "10000/10000 [==============================] - 4s 351us/sample - loss: 1.9569 - acc: 0.5798\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
      "312/312 [==============================] - 58s 185ms/step - loss: 2.5743 - acc: 0.5716 - val_loss: 1.9475 - val_acc: 0.5798\n",
      "Epoch 49/120\n",
      "10000/10000 [==============================] - 3s 349us/sample - loss: 1.6466 - acc: 0.6517\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.4012 - acc: 0.6256 - val_loss: 1.6380 - val_acc: 0.6517\n",
      "Epoch 50/120\n",
      "10000/10000 [==============================] - 4s 351us/sample - loss: 1.6515 - acc: 0.6526\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.3191 - acc: 0.6535 - val_loss: 1.6433 - val_acc: 0.6526\n",
      "Epoch 51/120\n",
      "10000/10000 [==============================] - 3s 349us/sample - loss: 1.6485 - acc: 0.6558\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.2760 - acc: 0.6627 - val_loss: 1.6413 - val_acc: 0.6558\n",
      "Epoch 52/120\n",
      "10000/10000 [==============================] - 4s 351us/sample - loss: 1.6114 - acc: 0.6593\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.2527 - acc: 0.6662 - val_loss: 1.6026 - val_acc: 0.6593\n",
      "Epoch 53/120\n",
      "10000/10000 [==============================] - 3s 349us/sample - loss: 1.5789 - acc: 0.6675\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.2345 - acc: 0.6698 - val_loss: 1.5692 - val_acc: 0.6675\n",
      "Epoch 54/120\n",
      "10000/10000 [==============================] - 3s 349us/sample - loss: 1.5808 - acc: 0.6598\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.2224 - acc: 0.6752 - val_loss: 1.5723 - val_acc: 0.6598\n",
      "Epoch 55/120\n",
      "10000/10000 [==============================] - 3s 350us/sample - loss: 1.5668 - acc: 0.6645\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 2.2028 - acc: 0.6771 - val_loss: 1.5591 - val_acc: 0.6645\n",
      "Epoch 56/120\n",
      "10000/10000 [==============================] - 3s 348us/sample - loss: 1.5966 - acc: 0.6570\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.1676 - acc: 0.6886 - val_loss: 1.5870 - val_acc: 0.6570\n",
      "Epoch 57/120\n",
      "10000/10000 [==============================] - 4s 351us/sample - loss: 1.5668 - acc: 0.6598\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 2.1696 - acc: 0.6833 - val_loss: 1.5581 - val_acc: 0.6598\n",
      "Epoch 58/120\n",
      "10000/10000 [==============================] - 4s 351us/sample - loss: 1.5826 - acc: 0.6599\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.1651 - acc: 0.6852 - val_loss: 1.5723 - val_acc: 0.6599\n",
      "Epoch 59/120\n",
      "10000/10000 [==============================] - 4s 353us/sample - loss: 1.5756 - acc: 0.6579\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.1483 - acc: 0.6921 - val_loss: 1.5673 - val_acc: 0.6579\n",
      "Epoch 60/120\n",
      "10000/10000 [==============================] - 3s 348us/sample - loss: 1.5457 - acc: 0.6634\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 2.1374 - acc: 0.6939 - val_loss: 1.5361 - val_acc: 0.6634\n",
      "Epoch 61/120\n",
      "10000/10000 [==============================] - 4s 351us/sample - loss: 1.5764 - acc: 0.6590\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.1343 - acc: 0.6919 - val_loss: 1.5658 - val_acc: 0.6590\n",
      "Epoch 62/120\n",
      "10000/10000 [==============================] - 4s 350us/sample - loss: 1.5527 - acc: 0.6596\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.1226 - acc: 0.6934 - val_loss: 1.5425 - val_acc: 0.6596\n",
      "Epoch 63/120\n",
      "10000/10000 [==============================] - 3s 349us/sample - loss: 1.5403 - acc: 0.6595\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.1097 - acc: 0.6932 - val_loss: 1.5325 - val_acc: 0.6595\n",
      "Epoch 64/120\n",
      "10000/10000 [==============================] - 3s 347us/sample - loss: 1.5877 - acc: 0.6553\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.1080 - acc: 0.6945 - val_loss: 1.5790 - val_acc: 0.6553\n",
      "Epoch 65/120\n",
      "10000/10000 [==============================] - 3s 349us/sample - loss: 1.5579 - acc: 0.6610\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.1038 - acc: 0.6950 - val_loss: 1.5475 - val_acc: 0.6610\n",
      "Epoch 66/120\n",
      "10000/10000 [==============================] - 3s 349us/sample - loss: 1.5738 - acc: 0.6548\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.0852 - acc: 0.6980 - val_loss: 1.5652 - val_acc: 0.6548\n",
      "Epoch 67/120\n",
      "10000/10000 [==============================] - 3s 349us/sample - loss: 1.5431 - acc: 0.6590\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.0905 - acc: 0.6951 - val_loss: 1.5338 - val_acc: 0.6590\n",
      "Epoch 68/120\n",
      "10000/10000 [==============================] - 3s 348us/sample - loss: 1.5277 - acc: 0.6639\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.0799 - acc: 0.7043 - val_loss: 1.5180 - val_acc: 0.6639\n",
      "Epoch 69/120\n",
      "10000/10000 [==============================] - 3s 349us/sample - loss: 1.5524 - acc: 0.6584\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.0775 - acc: 0.7041 - val_loss: 1.5420 - val_acc: 0.6584\n",
      "Epoch 70/120\n",
      "10000/10000 [==============================] - 3s 349us/sample - loss: 1.5205 - acc: 0.6644\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.0751 - acc: 0.6982 - val_loss: 1.5100 - val_acc: 0.6644\n",
      "Epoch 71/120\n",
      "10000/10000 [==============================] - 3s 337us/sample - loss: 1.5686 - acc: 0.6547\n",
      "312/312 [==============================] - 53s 168ms/step - loss: 2.0772 - acc: 0.6974 - val_loss: 1.5597 - val_acc: 0.6547\n",
      "Epoch 72/120\n",
      "10000/10000 [==============================] - 3s 337us/sample - loss: 1.5473 - acc: 0.6566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 52s 165ms/step - loss: 2.0698 - acc: 0.7014 - val_loss: 1.5412 - val_acc: 0.6566\n",
      "Epoch 73/120\n",
      "10000/10000 [==============================] - 3s 337us/sample - loss: 1.5016 - acc: 0.6670\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.0644 - acc: 0.7006 - val_loss: 1.4950 - val_acc: 0.6670\n",
      "Epoch 74/120\n",
      "10000/10000 [==============================] - 3s 334us/sample - loss: 1.5656 - acc: 0.6564\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.0597 - acc: 0.7029 - val_loss: 1.5589 - val_acc: 0.6564\n",
      "Epoch 75/120\n",
      "10000/10000 [==============================] - 3s 337us/sample - loss: 1.5464 - acc: 0.6558\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.0535 - acc: 0.7028 - val_loss: 1.5394 - val_acc: 0.6558\n",
      "Epoch 76/120\n",
      "10000/10000 [==============================] - 3s 336us/sample - loss: 1.5506 - acc: 0.6532\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.0353 - acc: 0.7071 - val_loss: 1.5431 - val_acc: 0.6532\n",
      "Epoch 77/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 1.6072 - acc: 0.6437\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.0467 - acc: 0.7044 - val_loss: 1.5984 - val_acc: 0.6437\n",
      "Epoch 78/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 1.5563 - acc: 0.6543\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.0339 - acc: 0.7051 - val_loss: 1.5490 - val_acc: 0.6543\n",
      "Epoch 79/120\n",
      "10000/10000 [==============================] - 3s 334us/sample - loss: 1.5422 - acc: 0.6545\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 2.0346 - acc: 0.7075 - val_loss: 1.5351 - val_acc: 0.6545\n",
      "Epoch 80/120\n",
      "10000/10000 [==============================] - 3s 334us/sample - loss: 1.4358 - acc: 0.6818\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.9636 - acc: 0.7358 - val_loss: 1.4282 - val_acc: 0.6818\n",
      "Epoch 81/120\n",
      "10000/10000 [==============================] - 3s 333us/sample - loss: 1.4277 - acc: 0.6867\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.9427 - acc: 0.7416 - val_loss: 1.4209 - val_acc: 0.6867\n",
      "Epoch 82/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 1.4347 - acc: 0.6817\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.9299 - acc: 0.7471 - val_loss: 1.4275 - val_acc: 0.6817\n",
      "Epoch 83/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 1.4184 - acc: 0.6863\n",
      "312/312 [==============================] - 51s 163ms/step - loss: 1.9198 - acc: 0.7487 - val_loss: 1.4109 - val_acc: 0.6863\n",
      "Epoch 84/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 1.4158 - acc: 0.6879\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.9145 - acc: 0.7508 - val_loss: 1.4091 - val_acc: 0.6879\n",
      "Epoch 85/120\n",
      "10000/10000 [==============================] - 3s 334us/sample - loss: 1.4262 - acc: 0.6831\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.9094 - acc: 0.7488 - val_loss: 1.4180 - val_acc: 0.6831\n",
      "Epoch 86/120\n",
      "10000/10000 [==============================] - 3s 333us/sample - loss: 1.4188 - acc: 0.6862\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.9000 - acc: 0.7533 - val_loss: 1.4111 - val_acc: 0.6862\n",
      "Epoch 87/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 1.4352 - acc: 0.6794\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8967 - acc: 0.7533 - val_loss: 1.4274 - val_acc: 0.6794\n",
      "Epoch 88/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 1.4232 - acc: 0.6863\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8875 - acc: 0.7555 - val_loss: 1.4155 - val_acc: 0.6863\n",
      "Epoch 89/120\n",
      "10000/10000 [==============================] - 3s 333us/sample - loss: 1.4181 - acc: 0.6860\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8775 - acc: 0.7579 - val_loss: 1.4091 - val_acc: 0.6860\n",
      "Epoch 90/120\n",
      "10000/10000 [==============================] - 3s 336us/sample - loss: 1.4216 - acc: 0.6859\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 1.8762 - acc: 0.7590 - val_loss: 1.4140 - val_acc: 0.6859\n",
      "Epoch 91/120\n",
      "10000/10000 [==============================] - 3s 334us/sample - loss: 1.4087 - acc: 0.6894\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8652 - acc: 0.7621 - val_loss: 1.4010 - val_acc: 0.6894\n",
      "Epoch 92/120\n",
      "10000/10000 [==============================] - 3s 334us/sample - loss: 1.4051 - acc: 0.6920\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8595 - acc: 0.7661 - val_loss: 1.3975 - val_acc: 0.6920\n",
      "Epoch 93/120\n",
      "10000/10000 [==============================] - 3s 333us/sample - loss: 1.4058 - acc: 0.6881\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8531 - acc: 0.7626 - val_loss: 1.3983 - val_acc: 0.6881\n",
      "Epoch 94/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 1.4103 - acc: 0.6898\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8694 - acc: 0.7619 - val_loss: 1.4031 - val_acc: 0.6898\n",
      "Epoch 95/120\n",
      "10000/10000 [==============================] - 3s 334us/sample - loss: 1.4058 - acc: 0.6887\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8646 - acc: 0.7682 - val_loss: 1.3984 - val_acc: 0.6887\n",
      "Epoch 96/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 1.4042 - acc: 0.6902\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8561 - acc: 0.7642 - val_loss: 1.3963 - val_acc: 0.6902\n",
      "Epoch 97/120\n",
      "10000/10000 [==============================] - 3s 333us/sample - loss: 1.4032 - acc: 0.6888\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8515 - acc: 0.7651 - val_loss: 1.3952 - val_acc: 0.6888\n",
      "Epoch 98/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 1.4005 - acc: 0.6906\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8618 - acc: 0.7659 - val_loss: 1.3927 - val_acc: 0.6906\n",
      "Epoch 99/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 1.4026 - acc: 0.6889\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8510 - acc: 0.7684 - val_loss: 1.3947 - val_acc: 0.6889\n",
      "Epoch 100/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 1.3974 - acc: 0.6903\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8536 - acc: 0.7683 - val_loss: 1.3897 - val_acc: 0.6903\n",
      "Epoch 101/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 1.4020 - acc: 0.6895\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8485 - acc: 0.7727 - val_loss: 1.3943 - val_acc: 0.6895\n",
      "Epoch 102/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 1.4007 - acc: 0.6900\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8578 - acc: 0.7643 - val_loss: 1.3932 - val_acc: 0.6900\n",
      "Epoch 103/120\n",
      "10000/10000 [==============================] - 3s 332us/sample - loss: 1.4002 - acc: 0.6918\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8511 - acc: 0.7667 - val_loss: 1.3926 - val_acc: 0.6918\n",
      "Epoch 104/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 1.4037 - acc: 0.6901\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8491 - acc: 0.7710 - val_loss: 1.3957 - val_acc: 0.6901\n",
      "Epoch 105/120\n",
      "10000/10000 [==============================] - 3s 337us/sample - loss: 1.4020 - acc: 0.6895\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8340 - acc: 0.7723 - val_loss: 1.3942 - val_acc: 0.6895\n",
      "Epoch 106/120\n",
      "10000/10000 [==============================] - 3s 334us/sample - loss: 1.4017 - acc: 0.6906\n",
      "\n",
      "Epoch 00106: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8385 - acc: 0.7704 - val_loss: 1.3938 - val_acc: 0.6906\n",
      "Epoch 107/120\n",
      "10000/10000 [==============================] - 3s 336us/sample - loss: 1.3995 - acc: 0.6913\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8293 - acc: 0.7732 - val_loss: 1.3915 - val_acc: 0.6913\n",
      "Epoch 108/120\n",
      "10000/10000 [==============================] - 3s 334us/sample - loss: 1.4009 - acc: 0.6898\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8425 - acc: 0.7727 - val_loss: 1.3930 - val_acc: 0.6898\n",
      "Epoch 109/120\n",
      "10000/10000 [==============================] - 3s 334us/sample - loss: 1.3981 - acc: 0.6905\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8454 - acc: 0.7726 - val_loss: 1.3902 - val_acc: 0.6905\n",
      "Epoch 110/120\n",
      "10000/10000 [==============================] - 3s 334us/sample - loss: 1.3987 - acc: 0.6898\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8340 - acc: 0.7739 - val_loss: 1.3908 - val_acc: 0.6898\n",
      "Epoch 111/120\n",
      "10000/10000 [==============================] - 3s 335us/sample - loss: 1.3983 - acc: 0.6905\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8413 - acc: 0.7695 - val_loss: 1.3904 - val_acc: 0.6905\n",
      "Epoch 112/120\n",
      "10000/10000 [==============================] - 3s 334us/sample - loss: 1.3991 - acc: 0.6911\n",
      "\n",
      "Epoch 00112: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-05.\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8415 - acc: 0.7737 - val_loss: 1.3911 - val_acc: 0.6911\n",
      "Epoch 113/120\n",
      "10000/10000 [==============================] - 3s 336us/sample - loss: 1.3994 - acc: 0.6909\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 1.8395 - acc: 0.7708 - val_loss: 1.3914 - val_acc: 0.6909\n"
     ]
    }
   ],
   "source": [
    "model = resnet.get_model()\n",
    "optimizer = SGD(lr=learning_rate, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "log= TensorBoard(log_dir=\"./logs/cm\")\n",
    "\n",
    "callbacks = [log,EarlyStopping(monitor = 'val_loss', patience = 13), \n",
    "             ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 6, verbose = 1,min_lr=1e-5)]\n",
    "\n",
    "datagen = ImageDataGenerator(width_shift_range=0.15,\n",
    "                             height_shift_range=0.15,\n",
    "                             horizontal_flip=True,\n",
    "                             preprocessing_function=cutout(cutout_mask_size = 10))\n",
    "\n",
    "training_generator = MixupGenerator(x_train, y_train, batch_size=batch_size, alpha=0.4, datagen=datagen)()\n",
    "model.fit(training_generator,\n",
    "          steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "          validation_data=(x_val, y_val),\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model.save('model.cm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdefa1f3",
   "metadata": {},
   "source": [
    "## cutout + cutmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5e9a199",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "10000/10000 [==============================] - 6s 612us/sample - loss: 5.8584 - acc: 0.0562\n",
      "312/312 [==============================] - 58s 187ms/step - loss: 4.7603 - acc: 0.0557 - val_loss: 5.8549 - val_acc: 0.0562\n",
      "Epoch 2/120\n",
      "10000/10000 [==============================] - 3s 345us/sample - loss: 3.8527 - acc: 0.1643\n",
      "312/312 [==============================] - 49s 159ms/step - loss: 4.4502 - acc: 0.1016 - val_loss: 3.8511 - val_acc: 0.1643\n",
      "Epoch 3/120\n",
      "10000/10000 [==============================] - 3s 348us/sample - loss: 3.6850 - acc: 0.1988\n",
      "312/312 [==============================] - 50s 160ms/step - loss: 4.2718 - acc: 0.1379 - val_loss: 3.6842 - val_acc: 0.1988\n",
      "Epoch 4/120\n",
      "10000/10000 [==============================] - 3s 347us/sample - loss: 3.2315 - acc: 0.2732\n",
      "312/312 [==============================] - 50s 160ms/step - loss: 4.1575 - acc: 0.1563 - val_loss: 3.2278 - val_acc: 0.2732\n",
      "Epoch 5/120\n",
      "10000/10000 [==============================] - 4s 351us/sample - loss: 3.2143 - acc: 0.2768\n",
      "312/312 [==============================] - 51s 162ms/step - loss: 4.0328 - acc: 0.1790 - val_loss: 3.2118 - val_acc: 0.2768\n",
      "Epoch 6/120\n",
      "10000/10000 [==============================] - 4s 350us/sample - loss: 2.9098 - acc: 0.3405\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 3.9340 - acc: 0.1937 - val_loss: 2.9051 - val_acc: 0.3405\n",
      "Epoch 7/120\n",
      "10000/10000 [==============================] - 3s 347us/sample - loss: 2.9548 - acc: 0.3249\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 3.8872 - acc: 0.2096 - val_loss: 2.9500 - val_acc: 0.3249\n",
      "Epoch 8/120\n",
      "10000/10000 [==============================] - 4s 351us/sample - loss: 2.8154 - acc: 0.3442\n",
      "312/312 [==============================] - 51s 164ms/step - loss: 3.8090 - acc: 0.2290 - val_loss: 2.8118 - val_acc: 0.3442\n",
      "Epoch 9/120\n",
      "10000/10000 [==============================] - 4s 351us/sample - loss: 2.7455 - acc: 0.3707\n",
      "312/312 [==============================] - 52s 165ms/step - loss: 3.7795 - acc: 0.2307 - val_loss: 2.7421 - val_acc: 0.3707\n",
      "Epoch 10/120\n",
      "10000/10000 [==============================] - 4s 357us/sample - loss: 2.5787 - acc: 0.3923\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 3.7194 - acc: 0.2480 - val_loss: 2.5729 - val_acc: 0.3923\n",
      "Epoch 11/120\n",
      "10000/10000 [==============================] - 4s 370us/sample - loss: 2.4967 - acc: 0.4142\n",
      "312/312 [==============================] - 54s 171ms/step - loss: 3.6172 - acc: 0.2676 - val_loss: 2.4934 - val_acc: 0.4142\n",
      "Epoch 12/120\n",
      "10000/10000 [==============================] - 4s 375us/sample - loss: 2.3831 - acc: 0.4459\n",
      "312/312 [==============================] - 54s 172ms/step - loss: 3.5485 - acc: 0.2804 - val_loss: 2.3802 - val_acc: 0.4459\n",
      "Epoch 13/120\n",
      "10000/10000 [==============================] - 4s 370us/sample - loss: 2.4431 - acc: 0.4258\n",
      "312/312 [==============================] - 54s 172ms/step - loss: 3.5274 - acc: 0.2922 - val_loss: 2.4370 - val_acc: 0.4258\n",
      "Epoch 14/120\n",
      "10000/10000 [==============================] - 4s 363us/sample - loss: 2.4466 - acc: 0.4327\n",
      "312/312 [==============================] - 53s 171ms/step - loss: 3.5378 - acc: 0.2879 - val_loss: 2.4430 - val_acc: 0.4327\n",
      "Epoch 15/120\n",
      "10000/10000 [==============================] - 4s 371us/sample - loss: 2.3603 - acc: 0.4495\n",
      "312/312 [==============================] - 54s 172ms/step - loss: 3.5392 - acc: 0.2900 - val_loss: 2.3541 - val_acc: 0.4495\n",
      "Epoch 16/120\n",
      "10000/10000 [==============================] - 4s 369us/sample - loss: 2.4651 - acc: 0.4306\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 3.4879 - acc: 0.2947 - val_loss: 2.4603 - val_acc: 0.4306\n",
      "Epoch 17/120\n",
      "10000/10000 [==============================] - 4s 365us/sample - loss: 2.3804 - acc: 0.4624\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 3.4392 - acc: 0.3083 - val_loss: 2.3750 - val_acc: 0.4624\n",
      "Epoch 18/120\n",
      "10000/10000 [==============================] - 4s 368us/sample - loss: 2.2756 - acc: 0.4680\n",
      "312/312 [==============================] - 54s 172ms/step - loss: 3.4117 - acc: 0.3148 - val_loss: 2.2727 - val_acc: 0.4680\n",
      "Epoch 19/120\n",
      "10000/10000 [==============================] - 4s 365us/sample - loss: 2.4915 - acc: 0.4242\n",
      "312/312 [==============================] - 53s 171ms/step - loss: 3.4014 - acc: 0.3228 - val_loss: 2.4905 - val_acc: 0.4242\n",
      "Epoch 20/120\n",
      "10000/10000 [==============================] - 4s 367us/sample - loss: 2.1898 - acc: 0.4880\n",
      "312/312 [==============================] - 53s 171ms/step - loss: 3.3483 - acc: 0.3329 - val_loss: 2.1865 - val_acc: 0.4880\n",
      "Epoch 21/120\n",
      "10000/10000 [==============================] - 4s 365us/sample - loss: 2.2858 - acc: 0.4648\n",
      "312/312 [==============================] - 53s 171ms/step - loss: 3.3967 - acc: 0.3297 - val_loss: 2.2789 - val_acc: 0.4648\n",
      "Epoch 22/120\n",
      "10000/10000 [==============================] - 4s 365us/sample - loss: 2.2279 - acc: 0.4827\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 3.3581 - acc: 0.3328 - val_loss: 2.2267 - val_acc: 0.4827\n",
      "Epoch 23/120\n",
      "10000/10000 [==============================] - 4s 366us/sample - loss: 2.3811 - acc: 0.4718\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 3.3599 - acc: 0.3315 - val_loss: 2.3774 - val_acc: 0.4718\n",
      "Epoch 24/120\n",
      "10000/10000 [==============================] - 4s 364us/sample - loss: 2.3055 - acc: 0.4755\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 3.3201 - acc: 0.3469 - val_loss: 2.3003 - val_acc: 0.4755\n",
      "Epoch 25/120\n",
      "10000/10000 [==============================] - 4s 363us/sample - loss: 2.1579 - acc: 0.4998\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 3.3264 - acc: 0.3461 - val_loss: 2.1529 - val_acc: 0.4998\n",
      "Epoch 26/120\n",
      "10000/10000 [==============================] - 4s 363us/sample - loss: 2.2043 - acc: 0.4858\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 3.2991 - acc: 0.3513 - val_loss: 2.2001 - val_acc: 0.4858\n",
      "Epoch 27/120\n",
      "10000/10000 [==============================] - 4s 363us/sample - loss: 2.1183 - acc: 0.5035\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 3.3374 - acc: 0.3430 - val_loss: 2.1120 - val_acc: 0.5035\n",
      "Epoch 28/120\n",
      "10000/10000 [==============================] - 4s 365us/sample - loss: 2.5510 - acc: 0.4318\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 3.2716 - acc: 0.3599 - val_loss: 2.5445 - val_acc: 0.4318\n",
      "Epoch 29/120\n",
      "10000/10000 [==============================] - 4s 366us/sample - loss: 2.3189 - acc: 0.4806\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 3.2433 - acc: 0.3679 - val_loss: 2.3146 - val_acc: 0.4806\n",
      "Epoch 30/120\n",
      "10000/10000 [==============================] - 4s 368us/sample - loss: 2.0592 - acc: 0.5293\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 3.2377 - acc: 0.3776 - val_loss: 2.0546 - val_acc: 0.5293\n",
      "Epoch 31/120\n",
      "10000/10000 [==============================] - 4s 365us/sample - loss: 2.1380 - acc: 0.5178\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 3.2897 - acc: 0.3626 - val_loss: 2.1321 - val_acc: 0.5178\n",
      "Epoch 32/120\n",
      "10000/10000 [==============================] - 4s 356us/sample - loss: 2.3846 - acc: 0.4699\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 3.2704 - acc: 0.3749 - val_loss: 2.3803 - val_acc: 0.4699\n",
      "Epoch 33/120\n",
      "10000/10000 [==============================] - 4s 355us/sample - loss: 2.2569 - acc: 0.5127\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 3.3245 - acc: 0.3417 - val_loss: 2.2512 - val_acc: 0.5127\n",
      "Epoch 34/120\n",
      "10000/10000 [==============================] - 4s 355us/sample - loss: 2.0339 - acc: 0.5294\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 3.2047 - acc: 0.3842 - val_loss: 2.0242 - val_acc: 0.5294\n",
      "Epoch 35/120\n",
      "10000/10000 [==============================] - 4s 357us/sample - loss: 2.2542 - acc: 0.5177\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 3.2759 - acc: 0.3609 - val_loss: 2.2480 - val_acc: 0.5177\n",
      "Epoch 36/120\n",
      "10000/10000 [==============================] - 4s 356us/sample - loss: 2.2696 - acc: 0.4900\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 3.2206 - acc: 0.3841 - val_loss: 2.2617 - val_acc: 0.4900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/120\n",
      "10000/10000 [==============================] - 4s 364us/sample - loss: 2.3862 - acc: 0.4741\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 3.2814 - acc: 0.3636 - val_loss: 2.3776 - val_acc: 0.4741\n",
      "Epoch 38/120\n",
      "10000/10000 [==============================] - 4s 363us/sample - loss: 2.1681 - acc: 0.5131\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 3.1877 - acc: 0.3925 - val_loss: 2.1590 - val_acc: 0.5131\n",
      "Epoch 39/120\n",
      "10000/10000 [==============================] - 4s 366us/sample - loss: 2.0511 - acc: 0.5463\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 3.2629 - acc: 0.3693 - val_loss: 2.0444 - val_acc: 0.5463\n",
      "Epoch 40/120\n",
      "10000/10000 [==============================] - 4s 374us/sample - loss: 2.1278 - acc: 0.5374\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.030000000447034835.\n",
      "312/312 [==============================] - 69s 222ms/step - loss: 3.2653 - acc: 0.3749 - val_loss: 2.1200 - val_acc: 0.5374\n",
      "Epoch 41/120\n",
      "10000/10000 [==============================] - 4s 361us/sample - loss: 1.8089 - acc: 0.6111\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 3.0592 - acc: 0.4256 - val_loss: 1.8020 - val_acc: 0.6111\n",
      "Epoch 42/120\n",
      "10000/10000 [==============================] - 4s 355us/sample - loss: 1.8473 - acc: 0.6112\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 2.9205 - acc: 0.4499 - val_loss: 1.8400 - val_acc: 0.6112\n",
      "Epoch 43/120\n",
      "10000/10000 [==============================] - 3s 348us/sample - loss: 1.8392 - acc: 0.6026\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.9445 - acc: 0.4354 - val_loss: 1.8312 - val_acc: 0.6026\n",
      "Epoch 44/120\n",
      "10000/10000 [==============================] - 3s 350us/sample - loss: 1.8394 - acc: 0.6023\n",
      "312/312 [==============================] - 52s 165ms/step - loss: 2.9233 - acc: 0.4623 - val_loss: 1.8311 - val_acc: 0.6023\n",
      "Epoch 45/120\n",
      "10000/10000 [==============================] - 3s 349us/sample - loss: 1.7159 - acc: 0.6119\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.9016 - acc: 0.4544 - val_loss: 1.7085 - val_acc: 0.6119\n",
      "Epoch 46/120\n",
      "10000/10000 [==============================] - 3s 348us/sample - loss: 1.8504 - acc: 0.5901\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.8713 - acc: 0.4585 - val_loss: 1.8419 - val_acc: 0.5901\n",
      "Epoch 47/120\n",
      "10000/10000 [==============================] - 4s 353us/sample - loss: 1.7290 - acc: 0.6169\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.8149 - acc: 0.4616 - val_loss: 1.7218 - val_acc: 0.6169\n",
      "Epoch 48/120\n",
      "10000/10000 [==============================] - 3s 347us/sample - loss: 1.8139 - acc: 0.6062\n",
      "312/312 [==============================] - 52s 165ms/step - loss: 2.8075 - acc: 0.4775 - val_loss: 1.8076 - val_acc: 0.6062\n",
      "Epoch 49/120\n",
      "10000/10000 [==============================] - 4s 353us/sample - loss: 1.7336 - acc: 0.6272\n",
      "312/312 [==============================] - 52s 165ms/step - loss: 2.8859 - acc: 0.4679 - val_loss: 1.7276 - val_acc: 0.6272\n",
      "Epoch 50/120\n",
      "10000/10000 [==============================] - 4s 358us/sample - loss: 1.6890 - acc: 0.6143\n",
      "312/312 [==============================] - 53s 168ms/step - loss: 2.8352 - acc: 0.4479 - val_loss: 1.6803 - val_acc: 0.6143\n",
      "Epoch 51/120\n",
      "10000/10000 [==============================] - 4s 357us/sample - loss: 1.7532 - acc: 0.6205\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 2.8964 - acc: 0.4591 - val_loss: 1.7451 - val_acc: 0.6205\n",
      "Epoch 52/120\n",
      "10000/10000 [==============================] - 3s 349us/sample - loss: 1.7017 - acc: 0.6162\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.8676 - acc: 0.4508 - val_loss: 1.6946 - val_acc: 0.6162\n",
      "Epoch 53/120\n",
      "10000/10000 [==============================] - 3s 348us/sample - loss: 1.7944 - acc: 0.5955\n",
      "312/312 [==============================] - 51s 165ms/step - loss: 2.8911 - acc: 0.4407 - val_loss: 1.7865 - val_acc: 0.5955\n",
      "Epoch 54/120\n",
      "10000/10000 [==============================] - 4s 355us/sample - loss: 1.8410 - acc: 0.6104\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 2.9196 - acc: 0.4420 - val_loss: 1.8348 - val_acc: 0.6104\n",
      "Epoch 55/120\n",
      "10000/10000 [==============================] - 4s 358us/sample - loss: 1.7274 - acc: 0.6118\n",
      "312/312 [==============================] - 52s 168ms/step - loss: 2.8733 - acc: 0.4347 - val_loss: 1.7212 - val_acc: 0.6118\n",
      "Epoch 56/120\n",
      "10000/10000 [==============================] - 4s 354us/sample - loss: 1.7691 - acc: 0.6102\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.009000000357627868.\n",
      "312/312 [==============================] - 52s 168ms/step - loss: 2.8701 - acc: 0.4537 - val_loss: 1.7619 - val_acc: 0.6102\n",
      "Epoch 57/120\n",
      "10000/10000 [==============================] - 4s 355us/sample - loss: 1.5737 - acc: 0.6550\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.7366 - acc: 0.4866 - val_loss: 1.5659 - val_acc: 0.6550\n",
      "Epoch 58/120\n",
      "10000/10000 [==============================] - 4s 385us/sample - loss: 1.5716 - acc: 0.6552\n",
      "312/312 [==============================] - 53s 171ms/step - loss: 2.6600 - acc: 0.4930 - val_loss: 1.5642 - val_acc: 0.6552\n",
      "Epoch 59/120\n",
      "10000/10000 [==============================] - 4s 358us/sample - loss: 1.5809 - acc: 0.6546\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 2.7102 - acc: 0.4822 - val_loss: 1.5741 - val_acc: 0.6546\n",
      "Epoch 60/120\n",
      "10000/10000 [==============================] - 4s 399us/sample - loss: 1.6208 - acc: 0.6463\n",
      "312/312 [==============================] - 53s 171ms/step - loss: 2.7472 - acc: 0.4717 - val_loss: 1.6129 - val_acc: 0.6463\n",
      "Epoch 61/120\n",
      "10000/10000 [==============================] - 4s 363us/sample - loss: 1.5713 - acc: 0.6620\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.6065 - acc: 0.5133 - val_loss: 1.5643 - val_acc: 0.6620\n",
      "Epoch 62/120\n",
      "10000/10000 [==============================] - 4s 355us/sample - loss: 1.6079 - acc: 0.6530\n",
      "312/312 [==============================] - 53s 168ms/step - loss: 2.6965 - acc: 0.4954 - val_loss: 1.6023 - val_acc: 0.6530\n",
      "Epoch 63/120\n",
      "10000/10000 [==============================] - 4s 354us/sample - loss: 1.5476 - acc: 0.6608\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 2.6169 - acc: 0.5100 - val_loss: 1.5403 - val_acc: 0.6608\n",
      "Epoch 64/120\n",
      "10000/10000 [==============================] - 5s 464us/sample - loss: 1.5365 - acc: 0.6617\n",
      "312/312 [==============================] - 54s 174ms/step - loss: 2.6376 - acc: 0.5156 - val_loss: 1.5297 - val_acc: 0.6617\n",
      "Epoch 65/120\n",
      "10000/10000 [==============================] - 4s 371us/sample - loss: 1.5248 - acc: 0.6664\n",
      "312/312 [==============================] - 55s 175ms/step - loss: 2.6719 - acc: 0.4879 - val_loss: 1.5173 - val_acc: 0.6664\n",
      "Epoch 66/120\n",
      "10000/10000 [==============================] - 4s 388us/sample - loss: 1.5637 - acc: 0.6574\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 2.6184 - acc: 0.5163 - val_loss: 1.5567 - val_acc: 0.6574\n",
      "Epoch 67/120\n",
      "10000/10000 [==============================] - 4s 361us/sample - loss: 1.5788 - acc: 0.6508\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 2.6060 - acc: 0.5086 - val_loss: 1.5709 - val_acc: 0.6508\n",
      "Epoch 68/120\n",
      "10000/10000 [==============================] - 4s 356us/sample - loss: 1.5666 - acc: 0.6589\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 2.6378 - acc: 0.5021 - val_loss: 1.5591 - val_acc: 0.6589\n",
      "Epoch 69/120\n",
      "10000/10000 [==============================] - 4s 359us/sample - loss: 1.5816 - acc: 0.6549\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.5910 - acc: 0.5245 - val_loss: 1.5753 - val_acc: 0.6549\n",
      "Epoch 70/120\n",
      "10000/10000 [==============================] - 4s 365us/sample - loss: 1.5395 - acc: 0.6667\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.5555 - acc: 0.5255 - val_loss: 1.5328 - val_acc: 0.6667\n",
      "Epoch 71/120\n",
      "10000/10000 [==============================] - 4s 358us/sample - loss: 1.5683 - acc: 0.6492\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.002700000163167715.\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 2.6350 - acc: 0.4994 - val_loss: 1.5631 - val_acc: 0.6492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/120\n",
      "10000/10000 [==============================] - 4s 354us/sample - loss: 1.5021 - acc: 0.6746\n",
      "312/312 [==============================] - 52s 168ms/step - loss: 2.5262 - acc: 0.5273 - val_loss: 1.4951 - val_acc: 0.6746\n",
      "Epoch 73/120\n",
      "10000/10000 [==============================] - 4s 350us/sample - loss: 1.4627 - acc: 0.6733\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 2.5088 - acc: 0.5280 - val_loss: 1.4556 - val_acc: 0.6733\n",
      "Epoch 74/120\n",
      "10000/10000 [==============================] - 4s 359us/sample - loss: 1.4950 - acc: 0.6735\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.5666 - acc: 0.5314 - val_loss: 1.4887 - val_acc: 0.6735\n",
      "Epoch 75/120\n",
      "10000/10000 [==============================] - 4s 359us/sample - loss: 1.4563 - acc: 0.6764\n",
      "312/312 [==============================] - 52s 168ms/step - loss: 2.5416 - acc: 0.5197 - val_loss: 1.4495 - val_acc: 0.6764\n",
      "Epoch 76/120\n",
      "10000/10000 [==============================] - 4s 356us/sample - loss: 1.4837 - acc: 0.6801\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.5008 - acc: 0.5399 - val_loss: 1.4770 - val_acc: 0.6801\n",
      "Epoch 77/120\n",
      "10000/10000 [==============================] - 3s 349us/sample - loss: 1.4590 - acc: 0.6798\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 2.5509 - acc: 0.5127 - val_loss: 1.4517 - val_acc: 0.6798\n",
      "Epoch 78/120\n",
      "10000/10000 [==============================] - 4s 356us/sample - loss: 1.5016 - acc: 0.6776\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.5497 - acc: 0.5143 - val_loss: 1.4952 - val_acc: 0.6776\n",
      "Epoch 79/120\n",
      "10000/10000 [==============================] - 4s 364us/sample - loss: 1.4512 - acc: 0.6781\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.5951 - acc: 0.5024 - val_loss: 1.4448 - val_acc: 0.6781\n",
      "Epoch 80/120\n",
      "10000/10000 [==============================] - 4s 364us/sample - loss: 1.4980 - acc: 0.6778\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 2.5036 - acc: 0.5378 - val_loss: 1.4918 - val_acc: 0.6778\n",
      "Epoch 81/120\n",
      "10000/10000 [==============================] - 4s 356us/sample - loss: 1.4953 - acc: 0.6776\n",
      "312/312 [==============================] - 52s 168ms/step - loss: 2.5886 - acc: 0.5140 - val_loss: 1.4881 - val_acc: 0.6776\n",
      "Epoch 82/120\n",
      "10000/10000 [==============================] - 4s 359us/sample - loss: 1.4648 - acc: 0.6770\n",
      "312/312 [==============================] - 52s 168ms/step - loss: 2.5046 - acc: 0.5247 - val_loss: 1.4583 - val_acc: 0.6770\n",
      "Epoch 83/120\n",
      "10000/10000 [==============================] - 4s 353us/sample - loss: 1.4434 - acc: 0.6840\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 2.4848 - acc: 0.5477 - val_loss: 1.4367 - val_acc: 0.6840\n",
      "Epoch 84/120\n",
      "10000/10000 [==============================] - 4s 354us/sample - loss: 1.4339 - acc: 0.6834\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 2.4041 - acc: 0.5732 - val_loss: 1.4274 - val_acc: 0.6834\n",
      "Epoch 85/120\n",
      "10000/10000 [==============================] - 3s 348us/sample - loss: 1.4971 - acc: 0.6764\n",
      "312/312 [==============================] - 52s 166ms/step - loss: 2.6120 - acc: 0.5035 - val_loss: 1.4911 - val_acc: 0.6764\n",
      "Epoch 86/120\n",
      "10000/10000 [==============================] - 4s 355us/sample - loss: 1.4648 - acc: 0.6787\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 2.4707 - acc: 0.5337 - val_loss: 1.4583 - val_acc: 0.6787\n",
      "Epoch 87/120\n",
      "10000/10000 [==============================] - 4s 355us/sample - loss: 1.4601 - acc: 0.6812\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.5332 - acc: 0.5244 - val_loss: 1.4534 - val_acc: 0.6812\n",
      "Epoch 88/120\n",
      "10000/10000 [==============================] - 4s 361us/sample - loss: 1.4311 - acc: 0.6862\n",
      "312/312 [==============================] - 53s 168ms/step - loss: 2.5015 - acc: 0.5478 - val_loss: 1.4243 - val_acc: 0.6862\n",
      "Epoch 89/120\n",
      "10000/10000 [==============================] - 4s 361us/sample - loss: 1.4638 - acc: 0.6788\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.5921 - acc: 0.5095 - val_loss: 1.4582 - val_acc: 0.6788\n",
      "Epoch 90/120\n",
      "10000/10000 [==============================] - 4s 381us/sample - loss: 1.4459 - acc: 0.6828\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 2.4788 - acc: 0.5285 - val_loss: 1.4397 - val_acc: 0.6828\n",
      "Epoch 91/120\n",
      "10000/10000 [==============================] - 4s 368us/sample - loss: 1.4441 - acc: 0.6836\n",
      "312/312 [==============================] - 53s 171ms/step - loss: 2.5026 - acc: 0.5425 - val_loss: 1.4374 - val_acc: 0.6836\n",
      "Epoch 92/120\n",
      "10000/10000 [==============================] - 4s 358us/sample - loss: 1.4352 - acc: 0.6870\n",
      "312/312 [==============================] - 53s 170ms/step - loss: 2.5216 - acc: 0.5173 - val_loss: 1.4285 - val_acc: 0.6870\n",
      "Epoch 93/120\n",
      "10000/10000 [==============================] - 3s 349us/sample - loss: 1.4088 - acc: 0.6824\n",
      "312/312 [==============================] - 52s 168ms/step - loss: 2.4419 - acc: 0.5567 - val_loss: 1.4019 - val_acc: 0.6824\n",
      "Epoch 94/120\n",
      "10000/10000 [==============================] - 4s 362us/sample - loss: 1.4295 - acc: 0.6846\n",
      "312/312 [==============================] - 53s 168ms/step - loss: 2.4466 - acc: 0.5440 - val_loss: 1.4235 - val_acc: 0.6846\n",
      "Epoch 95/120\n",
      "10000/10000 [==============================] - 4s 361us/sample - loss: 1.4773 - acc: 0.6822\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.5107 - acc: 0.5362 - val_loss: 1.4717 - val_acc: 0.6822\n",
      "Epoch 96/120\n",
      "10000/10000 [==============================] - 4s 356us/sample - loss: 1.4677 - acc: 0.6781\n",
      "312/312 [==============================] - 52s 168ms/step - loss: 2.5055 - acc: 0.5304 - val_loss: 1.4620 - val_acc: 0.6781\n",
      "Epoch 97/120\n",
      "10000/10000 [==============================] - 4s 369us/sample - loss: 1.4891 - acc: 0.6749\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.4660 - acc: 0.5358 - val_loss: 1.4832 - val_acc: 0.6749\n",
      "Epoch 98/120\n",
      "10000/10000 [==============================] - 4s 353us/sample - loss: 1.4693 - acc: 0.6764\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.4656 - acc: 0.5417 - val_loss: 1.4638 - val_acc: 0.6764\n",
      "Epoch 99/120\n",
      "10000/10000 [==============================] - 4s 353us/sample - loss: 1.4239 - acc: 0.6792\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 0.0008100000210106373.\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 2.4697 - acc: 0.5358 - val_loss: 1.4180 - val_acc: 0.6792\n",
      "Epoch 100/120\n",
      "10000/10000 [==============================] - 4s 356us/sample - loss: 1.4179 - acc: 0.6893\n",
      "312/312 [==============================] - 52s 168ms/step - loss: 2.4645 - acc: 0.5454 - val_loss: 1.4119 - val_acc: 0.6893\n",
      "Epoch 101/120\n",
      "10000/10000 [==============================] - 4s 353us/sample - loss: 1.4178 - acc: 0.6923\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 2.3896 - acc: 0.5673 - val_loss: 1.4118 - val_acc: 0.6923\n",
      "Epoch 102/120\n",
      "10000/10000 [==============================] - 4s 365us/sample - loss: 1.4465 - acc: 0.6867\n",
      "312/312 [==============================] - 53s 169ms/step - loss: 2.4333 - acc: 0.5321 - val_loss: 1.4409 - val_acc: 0.6867\n",
      "Epoch 103/120\n",
      "10000/10000 [==============================] - 4s 352us/sample - loss: 1.4466 - acc: 0.6840\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 2.5131 - acc: 0.5284 - val_loss: 1.4411 - val_acc: 0.6840\n",
      "Epoch 104/120\n",
      "10000/10000 [==============================] - 3s 348us/sample - loss: 1.4283 - acc: 0.6871\n",
      "312/312 [==============================] - 52s 165ms/step - loss: 2.4469 - acc: 0.5510 - val_loss: 1.4223 - val_acc: 0.6871\n",
      "Epoch 105/120\n",
      "10000/10000 [==============================] - 4s 356us/sample - loss: 1.4132 - acc: 0.6890\n",
      "\n",
      "Epoch 00105: ReduceLROnPlateau reducing learning rate to 0.00024299999931827186.\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 2.4438 - acc: 0.5549 - val_loss: 1.4073 - val_acc: 0.6890\n",
      "Epoch 106/120\n",
      "10000/10000 [==============================] - 4s 356us/sample - loss: 1.4214 - acc: 0.6893\n",
      "312/312 [==============================] - 52s 167ms/step - loss: 2.4237 - acc: 0.5539 - val_loss: 1.4154 - val_acc: 0.6893\n"
     ]
    }
   ],
   "source": [
    "model = resnet.get_model()\n",
    "optimizer = SGD(lr=learning_rate, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "log= TensorBoard(log_dir=\"./logs/cc\")\n",
    "\n",
    "callbacks = [log,EarlyStopping(monitor = 'val_loss', patience = 13), \n",
    "             ReduceLROnPlateau(monitor = 'val_loss', factor = 0.3, patience = 6, verbose = 1,min_lr=1e-5)]\n",
    "\n",
    "datagen = ImageDataGenerator(width_shift_range=0.15,\n",
    "                             height_shift_range=0.15,\n",
    "                             horizontal_flip=True,\n",
    "                            preprocessing_function=cutout(cutout_mask_size = 10))\n",
    "\n",
    "training_generator = CutMixGenerator(x_train, y_train, batch_size=batch_size, alpha=0.8, datagen=datagen)()\n",
    "model.fit(training_generator,\n",
    "          steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "          validation_data=(x_val, y_val),\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model.save('model_cc.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abe53f5",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4b49423",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 714us/sample - loss: 1.3716 - acc: 0.6971\n",
      "cutout+mixup : test loss 1.3716  test acc:0.6971\n",
      "\n",
      "10000/10000 [==============================] - 8s 754us/sample - loss: 1.4316 - acc: 0.6896\n",
      "cutout+cutmix : test loss 1.4316  test acc:0.6896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_cutmix=load_model(\"model.cutout+mixup.h5\")\n",
    "loss,acc=evaluate(model_cutmix,x_test,y_test)\n",
    "print(\"cutout+mixup : \"+prnt.format(loss,acc)+'\\n')\n",
    "\n",
    "model_cutmix=load_model(\"model_cutout+cutmix.h5\")\n",
    "loss,acc=evaluate(model_cutmix,x_test,y_test)\n",
    "print(\"cutout+cutmix : \"+prnt.format(loss,acc)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fcdcd2",
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
